# Key Players and Topics

# Non-Parametric Correlation {.unnumbered}

Correlation measures the strength and direction of association between two variables. While Pearson's correlation requires a linear relationship and normally distributed data, \emph{Spearman's rank correlation} and \emph{Kendall's tau} are \emph{non-parametric} measures, making them ideal for analyzing data that may not be linear or normally distributed.

## Spearman's Rank Correlation

Spearman's correlation coefficient $\rho$ is based on \emph{ranked} data. For two variables $X$ and $Y$, we replace each observation by its rank.

$\rho = 1 - \frac{6 \sum d^2}{n(n^2 - 1)}$

to compute Spearman's correlation coefficient $\rho$. This measure is based on ranked data. For two variables $X$ and $Y$, we first replace each observation by its rank. Once the data are ranked, Spearman's $\rho$ is computed similarly to Pearson's correlation, but using the ranks instead of the raw values:

$\rho = \frac{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})(r_{Y_i} - \bar{r}_{Y})}{\sqrt{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})^2}\sqrt{\sum_{i=1}^{n}(r_{Y_i} - \bar{r}_{Y})^2}}$

where $r_{X_i}$ is the rank of the $i$-th observation of $X$, $r_{Y_i}$ is the rank of the $i$-th observation of $Y$, and $\bar{r}_{X}$ and $\bar{r}_{Y}$ are the mean ranks of $X$ and $Y$, respectively.

Spearman's $\rho$ is then computed similarly to Pearson's correlation but on these ranks:

$\rho = \frac{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})(r_{Y_i} - \bar{r}_{Y})}{\sqrt{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})^2}\sqrt{\sum_{i=1}^{n}(r_{Y_i} - \bar{r}_{Y})^2}}$

where $r_{X_i}$ is the rank of the $i$-th observation of $X$, and $r_{Y_i}$ is the rank of the $i$-th observation of $Y$.

- A value of $\rho$ close to 1 indicates a strong positive monotonic relationship (as one variable increases, so does the other), while a value close to -1 indicates a strong negative monotonic relationship. A value around 0 suggests little or no monotonic association.

- This method is robust to non-normality and outliers since it relies on the order (ranks) rather than the actual values.

```{r}
#| include: false
source('setup.R')
```

```{r}
#| echo: true
#| message: false
#| warning: false

# Load required libraries
library(ggplot2)

# 1. Create a small dataset
x <- c(23, 42, 35, 44, 29)
y <- c(52, 31, 14, 23, 45)
data <- data.frame(x = x, y = y)

# 2. Compute the ranks for each variable
data$rank_x <- rank(data$x)  # For x: 1,2,3,4,5 (already sorted)
cat('r_X = ', data$rank_x)
data$rank_y <- rank(data$y)  # For y: gives ranks corresponding to [2,1,4,3,5]
cat('r_X = ', data$rank_y)

# 3. Compute differences between ranks and their squares
data$d <- data$rank_x - data$rank_y
data$d2 <- data$d^2

# Sum of squared differences
sum_d2 <- sum(data$d2)

# Number of observations
n <- nrow(data)

# 4. Calculate Spearman's correlation using the formula:
spearman_rho <- 1 - (6 * sum_d2) / (n * (n^2 - 1))

# Print computed Spearman correlation
print(paste("Spearman correlation (rho):", round(spearman_rho, 2)))
# Expected output: 0.8

# 5. Visualize the data using ggplot2

# Plot 1: Original Data Scatter Plot with a Linear Fit
p1 <- ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkblue") +
  ggtitle("Scatter Plot of Original Data") +
  xlab("x") + ylab("y") +
  theme_minimal()

# Plot 2: Scatter Plot of Ranks
p2 <- ggplot(data, aes(x = rank_x, y = rank_y)) +
  geom_point(color = "red", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  ggtitle("Scatter Plot of Ranks") +
  xlab("Rank of x") + ylab("Rank of y") +
  theme_minimal()

# Display the plots side by side
grid.arrange(p1, p2, ncol = 2)

# Optionally, print the data frame to show ranks and differences
print(data)
```

```{r}
#| echo: true
#| message: false
#| warning: false

delitos_data <- delitos_data %>% 
  select(-sum_24TR, -sum_24SE, -sum_24SS)

delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  cor(., method = "spearman", use = "complete.obs") %>%
  round(., 3) %>% 
  print(.) %>%
  corrplot(., method = "color", title = "Spearman Correlation", mar=c(0,0,1,0))
```

Both Spearman's $\rho$ captures the \emph{monotonic} relationship between two variables. They are more robust to outliers and non-linear relationships than Pearson's correlation. In the context of areal data (e.g., crime rates, population density across polygons), these measures can reveal how variables co-vary without assuming linearity or normality.

# Spatial Neighborhood Matrices {.unnumbered}

This section is based on *Spatial statistics for data science theory and practice with R*. See [@Moraga2023].

## Neighbors Based on Contiguity

-   Queen Contiguity: Two polygons are considered neighbors if they share any common point (i.e., an edge or a vertex). Mathematically, if polygons ( p_i ) and ( p_j ) touch at any point, then ( A\_{ij} = 1 ).
-   Rook Contiguity: Two polygons are neighbors only if they share a common edge. That is, if polygons ( p_i ) and ( p_j ) share a boundary segment, then ( A\_{ij} = 1 ); merely touching at a corner does not count.

```{r}
#| echo: true
#| message: false
#| warning: false

nb <- spdep::poly2nb(delitos_data, queen = TRUE)
head(nb)

nb_0 <- lapply(nb, function(x) if(length(x)==1 && x==0) integer(0) else x)

# Polygons with neighbors
table(sapply(nb_0, length))

# Neighbors of Order k Based on Contiguity
# Neighbors of second order
nblags <- spdep::nblag(neighbours = nb, maxlag = 2)
nblagsc <- spdep::nblag_cumul(nblags)
summary(nblagsc)
```

## Neighbors Based on k Nearest Neighbors

-   K-Nearest Neighbors: For each polygon, the ( k ) nearest neighbors are identified based on a distance threshold.
-   Distance Threshold: The distance threshold can be defined as a fixed value or as a function of the average distance between polygons.

```{r}
#| echo: true
#| message: false
#| warning: false

# Neighbors based on 3 nearest neighbors
coo <- st_centroid(delitos_data)
nb <- knn2nb(knearneigh(coo, k = 3)) # k number nearest neighbors

# Polygons with neighbors
table(sapply(nb, length))

# Subset data to the first 10 polygons
delitos_data_10 <- delitos_data[1:100, ]

# Recompute neighbor list for these 10 polygons to avoid index mismatches
nb_10 <- knn2nb(knearneigh(st_centroid(delitos_data_10), k = 3))

# Compute centroids for the 10 polygons
coords_10 <- st_coordinates(st_centroid(delitos_data_10))

# 4. Plot the first 10 polygons and overlay neighbor connections in red
plot(st_geometry(delitos_data_10), border = "lightgray", main = "First Polygons with 3 Nearest Neighbors")
plot.nb(nb_10, coords_10, add = TRUE, col = "red", lwd = 2)
```

## Neighbors Based on Distance

```{r}
#| echo: true
#| message: false
#| warning: false

nb <- dnearneigh(x = st_centroid(delitos_data), d1 = 0, d2 = 0.4)

# Polygons with neighbors
hist(sapply(nb, length))

# Subset data to the first 10 polygons
delitos_data_10 <- delitos_data[1:100, ]

# Recompute neighbor list for these 10 polygons to avoid index mismatches
nb_10 <- dnearneigh(x = st_centroid(delitos_data_10), d1 = 0, d2 = 0.4)

# Compute centroids for the 10 polygons
coords_10 <- st_coordinates(st_centroid(delitos_data_10))

# 4. Plot the first 10 polygons and overlay neighbor connections in red
plot(st_geometry(delitos_data_10), border = "lightgray", main = "First Polygons with 3 Nearest Neighbors")
plot.nb(nb_10, coords_10, add = TRUE, col = "red", lwd = 2)
```

Determining an Appropriate Upper Distance Bound: To ensure that each area in a spatial dataset has at least (k) neighbors, we can determine an appropriate upper distance bound by first computing the (k) nearest neighbors for each area. For example, using the Queen contiguity method, one may use the \texttt{spdep::knearneigh()} function with (k=1) to obtain the nearest neighbor for each polygon. This yields a matrix of neighbor IDs, which is then converted into a neighbor list (of class \texttt{nb}) via \texttt{knn2nb()}. Next, the \texttt{spdep::nbdists()} function computes the distances along the links between each area and its neighbor. By summarizing these distances (e.g., using \texttt{summary(unlist(dist1))}), we can observe the range of distances.

```{r}
#| echo: true
#| message: false
#| warning: false

# k is the number nearest neighbors
nb1 <- knn2nb(knearneigh(coo, k = 1))

dist1 <- nbdists(nb1, coo)

summary(unlist(dist1))

nb <- dnearneigh(x = st_centroid(delitos_data), d1 = 0, d2 = 1.2)

# Polygons with neighbors
hist(sapply(nb, length))
```

## Neighborhood Matrices

```{r}
#| echo: true
#| message: false
#| warning: false

# Spatial weights matrix based on a binary neighbor list
nb <- poly2nb(delitos_data, queen = TRUE)
nbw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)
nbw$weights[1:10]

# Spatial weights matrix based on inverse distance values
coo <- st_centroid(delitos_data)
nb <- poly2nb(delitos_data, queen = TRUE)
dists <- nbdists(nb, coo)
ids <- lapply(dists, function(x){1/x})
nbw <- nb2listw(nb, glist = ids, style = "B", zero.policy = TRUE)
nbw$weights[1:10]
```

# Spatial autocorrelation

# Correspondance Analysis {.unnumbered}

```{r}
#| eval: false
#| message: false
#| warning: false
#| include: false

# Paso 1: Crear la matriz de frecuencias de delitos en polígonos
Freq <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24'))

rownames(Freq) <- c("Zona Norte", "Zona Sur", "Zona Este", "Zona Oeste")
colnames(Freq) <- c("Homicidio", "Robo", "Hurto", "Extorsión")

print("Matriz de frecuencias:")
print(F)

# Paso 2: Calcular la tabla de frecuencias relativas
total_F <- sum(F)
F_rel <- F / total_F

print("Matriz de frecuencias relativas:")
print(F_rel)

# Paso 3: Cálculo de los totales por fila y columna
fi <- rowSums(F_rel)  # Suma por fila
fj <- colSums(F_rel)  # Suma por columna

D_f <- diag(fi)  # Matriz diagonal de totales de fila
D_c <- diag(fj)  # Matriz diagonal de totales de columna

# Paso 4: Construcción de la matriz de correspondencias Z
D_f_sqrt_inv <- diag(1 / sqrt(fi))  # Raíz cuadrada inversa de los totales de fila
D_c_sqrt_inv <- diag(1 / sqrt(fj))  # Raíz cuadrada inversa de los totales de columna

Z <- D_f_sqrt_inv %*% (F_rel - fi %o% fj) %*% D_c_sqrt_inv

print("Matriz de correspondencias Z:")
print(Z)

# Paso 5: Descomposición en Valores Singulares (SVD)
svd_result <- svd(Z)  # Descomposición de Z

U <- svd_result$u  # Vectores propios de ZZ' (Filas)
D <- diag(svd_result$d)  # Matriz diagonal de valores singulares
V <- svd_result$v  # Vectores propios de Z'Z (Columnas)

print("Matriz U (Vectores propios de ZZ'):")
print(U)

print("Matriz D (Valores singulares):")
print(D)

print("Matriz V (Vectores propios de Z'Z):")
print(V)

# Paso 6: Proyección de filas y columnas en el espacio reducido
dim_reducida <- 2
C_f <- D_f_sqrt_inv %*% U[, 1:dim_reducida] %*% D[1:dim_reducida, 1:dim_reducida]  # Coordenadas filas
C_c <- D_c_sqrt_inv %*% V[, 1:dim_reducida] %*% D[1:dim_reducida, 1:dim_reducida]  # Coordenadas columnas

print("Coordenadas filas en el espacio reducido:")
print(C_f)

print("Coordenadas columnas en el espacio reducido:")
print(C_c)

# Librerías
library(MASS)  # Para manipulación matricial
library(ca)  # Para el análisis de correspondencias

# Matriz de frecuencias (ejemplo: delitos en polígonos)
F <- matrix(c(
  12, 5, 8, 2,
  7, 3, 10, 1,
  4, 2, 6, 1,
  15, 8, 14, 5
), nrow = 4, byrow = TRUE)

rownames(F) <- c("Zona Norte", "Zona Sur", "Zona Este", "Zona Oeste")
colnames(F) <- c("Homicidio", "Robo", "Hurto", "Extorsión")

# Total de la tabla
n_total <- sum(F)

# Matriz de frecuencias relativas
F_rel <- F / n_total

# Totales por fila y columna
f_i <- rowSums(F_rel)
f_j <- colSums(F_rel)

# Matriz esperada bajo independencia
F_exp <- outer(f_i, f_j)

# Cálculo de ji-cuadrado
chi2 <- sum((F_rel - F_exp)^2 / F_exp) * n_total
print(paste("Estadístico Ji-cuadrado:", round(chi2, 4)))

# Distancia ji-cuadrado entre filas
D_c_inv <- diag(1 / f_j)  # Inversa de los totales de columnas
dist_ji <- as.matrix((F_rel - F_exp) %*% D_c_inv %*% t(F_rel - F_exp))
print("Matriz de distancias ji-cuadrado entre filas:")
print(dist_ji)
```

# Key Players and Topics

# Non-Parametric Correlation {.unnumbered}

Correlation measures the strength and direction of association between two variables. While Pearson's correlation requires a linear relationship and normally distributed data, \emph{Spearman's rank correlation} and \emph{Kendall's tau} are \emph{non-parametric} measures, making them ideal for analyzing data that may not be linear or normally distributed.

## Spearman's Rank Correlation

Spearman's correlation coefficient $\rho$ is based on \emph{ranked} data. For two variables $X$ and $Y$, we replace each observation by its rank.

$\rho = 1 - \frac{6 \sum d^2}{n(n^2 - 1)}$

to compute Spearman's correlation coefficient $\rho$. This measure is based on ranked data. For two variables $X$ and $Y$, we first replace each observation by its rank. Once the data are ranked, Spearman's $\rho$ is computed similarly to Pearson's correlation, but using the ranks instead of the raw values:

$\rho = \frac{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})(r_{Y_i} - \bar{r}_{Y})}{\sqrt{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})^2}\sqrt{\sum_{i=1}^{n}(r_{Y_i} - \bar{r}_{Y})^2}}$

where $r_{X_i}$ is the rank of the $i$-th observation of $X$, $r_{Y_i}$ is the rank of the $i$-th observation of $Y$, and $\bar{r}_{X}$ and $\bar{r}_{Y}$ are the mean ranks of $X$ and $Y$, respectively.

Spearman's $\rho$ is then computed similarly to Pearson's correlation but on these ranks:

$\rho = \frac{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})(r_{Y_i} - \bar{r}_{Y})}{\sqrt{\sum_{i=1}^{n}(r_{X_i} - \bar{r}_{X})^2}\sqrt{\sum_{i=1}^{n}(r_{Y_i} - \bar{r}_{Y})^2}}$

where $r_{X_i}$ is the rank of the $i$-th observation of $X$, and $r_{Y_i}$ is the rank of the $i$-th observation of $Y$.

-   A value of $\rho$ close to 1 indicates a strong positive monotonic relationship (as one variable increases, so does the other), while a value close to -1 indicates a strong negative monotonic relationship. A value around 0 suggests little or no monotonic association.

-   This method is robust to non-normality and outliers since it relies on the order (ranks) rather than the actual values.

```{r}
#| include: false
source('setup.R')
```

```{r}
#| echo: true
#| message: false
#| warning: false

# Load required libraries
library(ggplot2)

# 1. Create a small dataset
x <- c(23, 42, 35, 44, 29)
y <- c(52, 31, 14, 23, 45)
data <- data.frame(x = x, y = y)

# 2. Compute the ranks for each variable
data$rank_x <- rank(data$x)  # For x: 1,2,3,4,5 (already sorted)
cat('r_X = ', data$rank_x)
data$rank_y <- rank(data$y)  # For y: gives ranks corresponding to [2,1,4,3,5]
cat('r_X = ', data$rank_y)

# 3. Compute differences between ranks and their squares
data$d <- data$rank_x - data$rank_y
data$d2 <- data$d^2

# Sum of squared differences
sum_d2 <- sum(data$d2)

# Number of observations
n <- nrow(data)

# 4. Calculate Spearman's correlation using the formula:
spearman_rho <- 1 - (6 * sum_d2) / (n * (n^2 - 1))

# Print computed Spearman correlation
print(paste("Spearman correlation (rho):", round(spearman_rho, 2)))
# Expected output: 0.8

# 5. Visualize the data using ggplot2

# Plot 1: Original Data Scatter Plot with a Linear Fit
p1 <- ggplot(data, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkblue") +
  ggtitle("Scatter Plot of Original Data") +
  xlab("x") + ylab("y") +
  theme_minimal()

# Plot 2: Scatter Plot of Ranks
p2 <- ggplot(data, aes(x = rank_x, y = rank_y)) +
  geom_point(color = "red", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  ggtitle("Scatter Plot of Ranks") +
  xlab("Rank of x") + ylab("Rank of y") +
  theme_minimal()

# Display the plots side by side
grid.arrange(p1, p2, ncol = 2)

# Optionally, print the data frame to show ranks and differences
print(data)
```

```{r}
#| echo: true
#| message: false
#| warning: false

delitos_data <- delitos_data %>% 
  select(-sum_24TR, -sum_24SE, -sum_24SS)

delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  cor(., method = "spearman", use = "complete.obs") %>%
  round(., 3) %>% 
  print(.) %>%
  corrplot(., method = "color", title = "Spearman Correlation", mar=c(0,0,1,0))
```

Both Spearman's $\rho$ captures the \emph{monotonic} relationship between two variables. They are more robust to outliers and non-linear relationships than Pearson's correlation. In the context of areal data (e.g., crime rates, population density across polygons), these measures can reveal how variables co-vary without assuming linearity or normality.

# Spatial Neighborhood Matrices {.unnumbered}

This section is based on *Spatial statistics for data science theory and practice with R*. See [@Moraga2023].

## Neighbors Based on Contiguity

-   Queen Contiguity: Two polygons are considered neighbors if they share any common point (i.e., an edge or a vertex). Mathematically, if polygons $p_i$ and $p_j$ touch at any point, then $A\_{ij} = 1$.
-   Rook Contiguity: Two polygons are neighbors only if they share a common edge. That is, if polygons $p_i$ and $p_j$ share a boundary segment, then $A\_{ij} = 1$; merely touching at a corner does not count.

[![Neighbors based on contiguity. Area of interest is represented in black and its neighbors in gray.](images/arealdata-orderk-1.png){fig-align="center"}](Neighbors%20based%20on%20contiguity.%20Area%20of%20interest%20is%20represented%20in%20black%20and%20its%20neighbors%20in%20gray.)

```{r}
#| echo: true
#| message: false
#| warning: false

# Create a spatial neighbors list using Queen contiguity
# (i.e., polygons are considered neighbors if they share any point: edge or vertex)
nb <- spdep::poly2nb(delitos_data, queen = TRUE)
head(nb)

# Replace invalid neighbor entries (i.e., [1] 0) with empty integer vectors
# This ensures compatibility with functions that expect valid neighbor lists only
nb_0 <- lapply(nb, function(x) if(length(x)==1 && x==0) integer(0) else x)

# Polygons with neighbors
table(sapply(nb_0, length))

# Neighbors of Order k Based on Contiguity
# Neighbors of second order
nblags <- spdep::nblag(neighbours = nb, maxlag = 2)

# Combine neighbors of all orders up to the specified lag (in this case, up to order 2)
# This creates a cumulative neighbor list including first- and second-order neighbors
nblagsc <- spdep::nblag_cumul(nblags)
table(sapply(nblagsc, length))
```

## Neighbors Based on k Nearest Neighbors

-   K-Nearest Neighbors: For each polygon, the ( k ) nearest neighbors are identified based on a distance threshold.
-   Distance Threshold: The distance threshold can be defined as a fixed value or as a function of the average distance between polygons.

\textbf{k-Nearest Neighbors (kNN)} is a method that defines neighbors based on distance rather than contiguity. For each spatial unit $p_i$, the $k$ closest units (according to Euclidean distance or other metric) are selected as neighbors.

Formally, let $D(p_i, p_j)$ be the distance between polygons $p_i$ and $p_j$. Then, the neighbor set $N_k(p_i)$ is defined as:

$N_k(p_i) = p_j$ : $p_j$ is among the $k$ nearest polygons to $p_i$.

This ensures that each polygon has exactly $k$ neighbors, which is useful when spatial units are irregular or disconnected.

[![We define spatial k-nearest neighbour problem as finding k observations from a set of candidates C that are the most similar to the given a landmark L\[i\], where the similarity is defined by a distance function d(L\[i\], S\[j\]) = st_distance(L\[i\], S\[j\])](images/nearest_neighbour.png){fig-align="center"}](https://databrickslabs.github.io/mosaic/models/spatial-knn.html)

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute centroids of the polygons
coo <- st_centroid(delitos_data)

# Create a neighbor list where each polygon (based on its centroid `coo`) is connected 
# to its 3 nearest neighbors using k-nearest neighbors (k = 3)
nb <- knn2nb(knearneigh(coo, k = 3)) # k number nearest neighbors

# Polygons with neighbors
table(sapply(nb, length))

# Subset data to the first 10 polygons
delitos_data_10 <- delitos_data[1:100, ]

# Recompute neighbor list for these 10 polygons to avoid index mismatches
nb_10 <- knn2nb(knearneigh(st_centroid(delitos_data_10), k = 3))

# Compute centroids for the 10 polygons
coords_10 <- st_coordinates(st_centroid(delitos_data_10))

# Plot the first 10 polygons and overlay neighbor connections in red
plot(st_geometry(delitos_data_10), border = "lightgray", main = "First Polygons with 3 Nearest Neighbors")
plot.nb(nb_10, coords_10, add = TRUE, col = "red", lwd = 2)
```

## Neighbors Based on Distance

```{r}
#| echo: true
#| message: false
#| warning: false

# Create a neighbor list using distance-based contiguity:
# Polygons are considered neighbors if their centroids are within 0.4 units (e.g., degrees) apart
nb <- dnearneigh(x = st_centroid(delitos_data), d1 = 0, d2 = 0.4)

# Polygons with neighbors
hist(sapply(nb, length))

# Subset data to the first 10 polygons
delitos_data_10 <- delitos_data[1:100, ]

# Recompute neighbor list for these 10 polygons to avoid index mismatches
nb_10 <- dnearneigh(x = st_centroid(delitos_data_10), d1 = 0, d2 = 0.4)

# Compute centroids for the 10 polygons
coords_10 <- st_coordinates(st_centroid(delitos_data_10))

# Plot the first 10 polygons and overlay neighbor connections in red
plot(st_geometry(delitos_data_10), border = "lightgray", main = "First Polygons with 3 Nearest Neighbors")
plot.nb(nb_10, coords_10, add = TRUE, col = "red", lwd = 2)
```

Determining an Appropriate Upper Distance Bound: To ensure that each area in a spatial dataset has at least (k) neighbors, we can determine an appropriate upper distance bound by first computing the (k) nearest neighbors for each area. For example, using the Queen contiguity method, one may use the \textit{spdep::knearneigh()} function with (k=1) to obtain the nearest neighbor for each polygon. This yields a matrix of neighbor IDs, which is then converted into a neighbor list (of class \textit{nb}) via \textit{knn2nb()}. Next, the \textit{spdep::nbdists()} function computes the distances along the links between each area and its neighbor. By summarizing these distances (e.g., using \textit{summary(unlist(dist1))}), we can observe the range of distances.

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute k-nearest neighbors: for each polygon centroid, find its 1 nearest neighbor (k = 1)
nb1 <- knn2nb(knearneigh(coo, k = 1))

# Calculate the Euclidean distances between each polygon and its nearest neighbor
dist1 <- nbdists(nb1, coo)

# Summarize all distances to understand the minimum, maximum, and quartiles
summary(unlist(dist1))

# Create a distance-based neighbor list: polygons whose centroids are within [0, 1.2] units are considered neighbors
nb <- dnearneigh(x = st_centroid(delitos_data), d1 = 0, d2 = 1.2)

# Polygons with neighbors
hist(sapply(nb, length))
```

## Neighborhood Matrices

```{r}
#| echo: true
#| message: false
#| warning: false

# Spatial weights matrix using Queen contiguity (binary weights)
# 'queen = TRUE' considers shared edges OR vertices as neighbors
nb <- poly2nb(delitos_data, queen = TRUE)

# Convert the neighbor list to a spatial weights list object
# 'style = "W"' row-standardizes the weights (sums to 1)
# 'zero.policy = TRUE' avoids errors when some polygons have no neighbors
nbw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)

# Display the first 10 rows of spatial weights (for the first 10 polygons)
nbw$weights[1:10]

# Spatial weights matrix based on inverse distance values
# Compute centroids of polygons
coo <- st_centroid(delitos_data)

# Use Queen contiguity again to define neighbors
nb <- poly2nb(delitos_data, queen = TRUE)

# Compute distances between neighbors based on their centroids
dists <- nbdists(nb, coo)

# Create inverse distance weights (1/distance) for each pair of neighbors
ids <- lapply(dists, function(x){1/x})

# Create a listw object using binary style ("B" = no standardization)
nbw <- nb2listw(nb, glist = ids, style = "B", zero.policy = TRUE)

# Display the first 10 inverse-distance-based weights
nbw$weights[1:10]
```

# Spatial autocorrelation {.unnumbered}

## Global Moran’s

Spatial Weights Matrix (W)

The \textit{nb2listw(nb, style = "W")} function calculates the spatial weights matrix, often denoted as $W$. This matrix defines the spatial relationships between the polygons. The \textit{style = "W"} argument specifies row-standardization.

-   Let $w_{ij}$ be an element of the matrix $W$.
-   If polygon $i$ is a neighbor of polygon $j$, then $w_{ij} = \frac{1}{n_i}$, where $n_i$ is the number of neighbors of polygon $i$.
-   If polygon $i$ is not a neighbor of polygon $j$, then $w_{ij} = 0$.
-   The diagonal elements, $w_{ii}$, are typically 0 (a polygon is not considered a neighbor of itself).

In essence, each row of the matrix $W$ represents a polygon, and the entries in that row represent the influence of its neighbors. Row-standardization means that the elements in each row sum to 1.

Global Moran's I Statistic: \textit{moran.test()} function calculates Moran's I, a measure of global spatial autocorrelation. The formula for Moran's I is:

$I = \frac{n \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} (x_i - \bar{x})(x_j - \bar{x})}{(\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij}) \sum_{i=1}^{n} (x_i - \bar{x})^2}$

Where:

-   $n$ is the number of observations (polygons).
-   $x_i$ is the value of the variable of interest (in your case, \texttt{delitos\_data\$sum\_24HP}) for polygon $i$.
-   $\bar{x}$ is the mean of the variable $x$.
-   $w_{ij}$ is the spatial weight between polygon $i$ and polygon $j$ from the matrix $W$.

Simplified:

$I = \frac{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} (x_i - \bar{x})(x_j - \bar{x})}{S^2 \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij}}$

Where

$S^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2$

Moran's I essentially measures the correlation between the values at a location and the values at neighboring locations.

Interpretation of Moran's I:

-   $I$ ranges from -1 to +1.
-   $I > 0$: Positive spatial autocorrelation. Similar values tend to cluster together.
-   $I < 0$: Negative spatial autocorrelation. Dissimilar values tend to cluster together.
-   $I \approx 0$: Random spatial pattern.

Hypothesis Testing:

The \textit{moran.test()} function also performs a hypothesis test to assess the statistical significance of the observed spatial pattern.

-   \textit{Null Hypothesis ($H_0$):} The variable is randomly distributed in space (no spatial autocorrelation).
-   \textif{Alternative Hypothesis ($H_a$):} There is spatial autocorrelation (you specified \textit{alternative = "greater"}, so it's testing for \textit{positive} spatial autocorrelation).

Output of \textit{moran.test()}

-   \textit{gmoran[["estimate"]][["Moran I statistic"]]}: The calculated value of Moran's I.
-   \textit{gmoran[["statistic"]]}: The z-score, which measures how far the observed Moran's I is from the expected value under the null hypothesis, in standard deviations.
-   \textit{gmoran[["p.value"]]}: The p-value, which is the probability of observing a Moran's 
    
I value as extreme as, or more extreme than, the one calculated, assuming the null hypothesis is true. A small p-value (typically less than 0.05) suggests that you can reject the null hypothesis and conclude that there is statistically significant spatial autocorrelation.

```{r}
# Compute centroids of the polygons
coo <- st_centroid(delitos_data)

# Create a neighbor list where each polygon (based on its centroid `coo`) is connected 
# to its 3 nearest neighbors using k-nearest neighbors (k = 3)
nb <- knn2nb(knearneigh(coo, k = 3)) # k number nearest neighbors

# Global Moran's I
# Convert the neighbor list to a listw object
lw <- nb2listw(nb, style = "W") # Use nb2listw

# Now you can use 'lw' in moran.test
gmoran <- moran.test(delitos_data$sum_24HP, lw, alternative = "greater")

gmoran

gmoran[["estimate"]][["Moran I statistic"]] # Moran's I

gmoran[["statistic"]] # z-score

gmoran[["p.value"]] # p-value
```

-   `gmoran` The p-value (0.01282) is less than the significance level of 0.05, so we reject the null hypothesis. This means that there is statistically significant positive spatial autocorrelation in the data. In other words, the values of delitos_data$sum_24HP show a tendency to cluster together geographically. The positive z-score (2.2318) further supports this conclusion. It indicates that the observed Moran's I is significantly higher than what would be expected by chance if the values were randomly distributed.
-   `gmoran[["estimate"]][["Moran I statistic"]]`The value you got, 0.004845791, is a small positive number. Here's what that means: Positive Spatial Autocorrelation: The fact that it's positive indicates there's a slight tendency for values at nearby locations to be similar. In summary, there's a very slight indication that similar values are located near each other, but the effect is not very pronounced.
-   `gmoran[["statistic"]]` The z-score of 2.2318 indicates how many standard deviations the observed Moran's I is from the expected value under the null hypothesis. A positive z-score suggests that the observed spatial pattern is more clustered than expected by chance. 
-   `gmoran[["p.value"]]` The p-value of 0.01282 indicates the probability of observing a Moran's I value as extreme as the one calculated, assuming the null hypothesis is true. Since this p-value is less than the common significance level of 0.05, it suggests that the observed spatial pattern is statistically significant.


\textit{Moran's I Monte Carlo Simulation:} The code you provided performs a Monte Carlo simulation to assess the significance of Moran's I.  Here's a breakdown:

Moran's I Monte Carlo Test: The function `moran.mc` performs a Monte Carlo simulation of Moran's I.  Instead of relying on the theoretical distribution of Moran's I (which can be complex), it generates a set of random spatial patterns to create an empirical distribution.

```{r}
#| echo: true
#| message: false
#| warning: false
gmoranMC <- moran.mc(delitos_data$sum_24HP, lw, nsim = 99)
gmoranMC

hist(gmoranMC$res)
abline(v = gmoranMC$statistic, col = "red")
```

```{r}
#| echo: true
#| message: false
#| warning: false

moran.plot(delitos_data$sum_24HP, lw)
```

## Local Moran’s I

```{r}
#| echo: true
#| message: false
#| warning: false

lmoran <- localmoran(delitos_data$sum_24HP, lw, alternative = "greater")
head(lmoran)
```

Let's go through those columns again, but this time, I'll emphasize what they tell you about the pattern at each individual location:

Ii (Local Moran's I):

-   Big positive number: This neighborhood has a crime rate similar to its neighbors. If its crime rate is high, its neighbors also tend to have high crime rates (a "hot spot"). If its crime rate is low, its neighbors also tend to have low crime rates (a "cold spot").
-   Big negative number: This neighborhood has a crime rate very different from its neighbors. A high-crime neighborhood surrounded by low-crime neighborhoods, or vice versa (a "spatial outlier").
-   Number close to zero: This neighborhood's crime rate isn't particularly similar to or different from its neighbors. There's no clear local pattern.

E.Ii (Expected Ii):

-   This is what Ii would be on average if crime rates were randomly distributed. It's usually a very small number, close to zero. You don't focus on this column too much for direct interpretation.

Var.Ii (Variance of Ii):

-   This tells you how much the Local Moran's I values for this type of situation tend to vary. A larger variance means the Ii value can fluctuate more due to chance. You don't interpret this column directly.

Z.Ii (Z-score of Ii):

-   This is the most important column for determining if the pattern at a location is statistically significant. It tells you how "unusual" the Ii value is.
-   Big positive or negative number (outside -1.96 to +1.96): This neighborhood has a statistically significant pattern. It's either a hot spot, a cold spot, or a spatial outlier.
-   Number close to zero (inside -1.96 to +1.96): The pattern at this neighborhood could easily have happened by chance. It's not statistically significant.

Pr(z > E(Ii)) (P-value):

-   This is another way to determine statistical significance.
-   Small number (less than 0.05): This neighborhood has a statistically significant pattern (hot spot, cold spot, or spatial outlier).
-   Large number (greater than 0.05): The pattern at this neighborhood could easily have happened by chance. It's not statistically significant.


```{r}
#| eval: false
#| message: false
#| warning: false

tmap_mode("view")

delitos_data$lmI <- lmoran[, "Ii"] # local Moran's I
delitos_data$lmZ <- lmoran[, "Z.Ii"] # z-scores
# p-values corresponding to alternative greater
delitos_data$lmp <- lmoran[, "Pr(z > E(Ii))"]

p1 <- tm_shape(delitos_data) +
  tm_polygons(col = "sum_24HP", title = "vble", style = "quantile") +
  tm_layout(legend.outside = TRUE)

p2 <- tm_shape(delitos_data) +
  tm_polygons(col = "lmI", title = "Local Moran's I",
              style = "quantile") +
  tm_layout(legend.outside = TRUE)

p3 <- tm_shape(delitos_data) +
  tm_polygons(col = "lmZ", title = "Z-score",
              breaks = c(-Inf, 1.65, Inf)) +
  tm_layout(legend.outside = TRUE)

p4 <- tm_shape(delitos_data) +
  tm_polygons(col = "lmp", title = "p-value",
              breaks = c(-Inf, 0.05, Inf)) +
  tm_layout(legend.outside = TRUE)

tmap_arrange(p1, p2, p3, p4)
```

p1: sum_24HP (Your Variable)

This map shows the distribution of your variable of interest (sum_24HP).  The style = "quantile" argument means the colors represent quantiles, so each color shows roughly an equal number of polygons.  This map gives you a baseline for seeing how your variable is distributed spatially.

p2: lmI (Local Moran's I)

This map displays the Local Moran's I values.

Positive values (often shown in warmer colors) indicate that a polygon has neighbors with similar values.  This suggests spatial clusters of high values or low values.

Negative values (often shown in cooler colors) indicate that a polygon has neighbors with dissimilar values.  This suggests spatial outliers.

Again, style = "quantile" is used, so the colors represent the distribution of Local Moran's I values.

p3: lmZ (Z-scores)

This map shows the z-scores associated with the Local Moran's I values.  Z-scores help you assess the statistical significance of the local clustering.

You've set breaks = c(-Inf, 1.65, Inf).  Assuming this is a one-tailed test:

Values greater than 1.65 indicate statistically significant (at approximately the 0.05 level) clustering of high values (a "hot spot").

Values less than -1.65 would indicate statistically significant clustering of low values (a "cold spot").

Values between -1.65 and 1.65 suggest the spatial pattern is not statistically significant.

p4: lmp (P-values)

This map displays the p-values associated with the Local Moran's I values.  P-values provide another way to assess statistical significance.

You've set breaks = c(-Inf, 0.05, Inf).

Polygons with p-values less than 0.05 (shown in one color) indicate statistically significant spatial clustering.

Polygons with p-values greater than 0.05 (shown in the other color) do not show statistically significant clustering.


```{r}
#| echo: true
#| message: false
#| warning: false

# Plot Local Moran's I using tmap
tm_shape(delitos_data) +
    tm_polygons(
        col = "lmZ",
        title = "Local Moran's I",
        style = "fixed",
        breaks = c(-Inf, -1.96, 1.96, Inf),
        labels = c("Negative SAC", "No SAC", "Positive SAC"),
        palette = c("blue", "white", "red")
    ) +
    tm_layout(legend.outside = TRUE)
```

## Clusters

```{r}
#| echo: true
#| message: false
#| warning: false

mp <- moran.plot(as.vector(scale(delitos_data$sum_24HP)), lw)
delitos_data$quadrant <- NA
# high-high
delitos_data[(mp$x >= 0 & mp$wx >= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 1
# low-low
delitos_data[(mp$x <= 0 & mp$wx <= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 2
# high-low
delitos_data[(mp$x >= 0 & mp$wx <= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 3
# low-high
delitos_data[(mp$x <= 0 & mp$wx >= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 4
# non-significant
delitos_data[(delitos_data$lmp > 0.05), "quadrant"] <- 5
```

```{r}
#| echo: true
#| message: false
#| warning: false

tm_shape(delitos_data) + tm_fill(col = "quadrant", title = "",
breaks = c(1, 2, 3, 4, 5, 6),
palette =  c("red", "blue", "lightpink", "skyblue2", "white"),
labels = c("High-High", "Low-Low", "High-Low",
           "Low-High", "Non-significant")) +
tm_legend(text.size = 1)  + tm_borders(alpha = 0.5) +
tm_layout(frame = FALSE,  title = "Clusters")  +
tm_layout(legend.outside = TRUE)
```

# Correspondance Analysis {.unnumbered}

```{r}
#| eval: false
#| message: false
#| warning: false
#| include: false

# Paso 1: Crear la matriz de frecuencias de delitos en polígonos
Freq <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24'))

rownames(Freq) <- c("Zona Norte", "Zona Sur", "Zona Este", "Zona Oeste")
colnames(Freq) <- c("Homicidio", "Robo", "Hurto", "Extorsión")

print("Matriz de frecuencias:")
print(F)

# Paso 2: Calcular la tabla de frecuencias relativas
total_F <- sum(F)
F_rel <- F / total_F

print("Matriz de frecuencias relativas:")
print(F_rel)

# Paso 3: Cálculo de los totales por fila y columna
fi <- rowSums(F_rel)  # Suma por fila
fj <- colSums(F_rel)  # Suma por columna

D_f <- diag(fi)  # Matriz diagonal de totales de fila
D_c <- diag(fj)  # Matriz diagonal de totales de columna

# Paso 4: Construcción de la matriz de correspondencias Z
D_f_sqrt_inv <- diag(1 / sqrt(fi))  # Raíz cuadrada inversa de los totales de fila
D_c_sqrt_inv <- diag(1 / sqrt(fj))  # Raíz cuadrada inversa de los totales de columna

Z <- D_f_sqrt_inv %*% (F_rel - fi %o% fj) %*% D_c_sqrt_inv

print("Matriz de correspondencias Z:")
print(Z)

# Paso 5: Descomposición en Valores Singulares (SVD)
svd_result <- svd(Z)  # Descomposición de Z

U <- svd_result$u  # Vectores propios de ZZ' (Filas)
D <- diag(svd_result$d)  # Matriz diagonal de valores singulares
V <- svd_result$v  # Vectores propios de Z'Z (Columnas)

print("Matriz U (Vectores propios de ZZ'):")
print(U)

print("Matriz D (Valores singulares):")
print(D)

print("Matriz V (Vectores propios de Z'Z):")
print(V)

# Paso 6: Proyección de filas y columnas en el espacio reducido
dim_reducida <- 2
C_f <- D_f_sqrt_inv %*% U[, 1:dim_reducida] %*% D[1:dim_reducida, 1:dim_reducida]  # Coordenadas filas
C_c <- D_c_sqrt_inv %*% V[, 1:dim_reducida] %*% D[1:dim_reducida, 1:dim_reducida]  # Coordenadas columnas

print("Coordenadas filas en el espacio reducido:")
print(C_f)

print("Coordenadas columnas en el espacio reducido:")
print(C_c)

# Librerías
library(MASS)  # Para manipulación matricial
library(ca)  # Para el análisis de correspondencias

# Matriz de frecuencias (ejemplo: delitos en polígonos)
F <- matrix(c(
  12, 5, 8, 2,
  7, 3, 10, 1,
  4, 2, 6, 1,
  15, 8, 14, 5
), nrow = 4, byrow = TRUE)

rownames(F) <- c("Zona Norte", "Zona Sur", "Zona Este", "Zona Oeste")
colnames(F) <- c("Homicidio", "Robo", "Hurto", "Extorsión")

# Total de la tabla
n_total <- sum(F)

# Matriz de frecuencias relativas
F_rel <- F / n_total

# Totales por fila y columna
f_i <- rowSums(F_rel)
f_j <- colSums(F_rel)

# Matriz esperada bajo independencia
F_exp <- outer(f_i, f_j)

# Cálculo de ji-cuadrado
chi2 <- sum((F_rel - F_exp)^2 / F_exp) * n_total
print(paste("Estadístico Ji-cuadrado:", round(chi2, 4)))

# Distancia ji-cuadrado entre filas
D_c_inv <- diag(1 / f_j)  # Inversa de los totales de columnas
dist_ji <- as.matrix((F_rel - F_exp) %*% D_c_inv %*% t(F_rel - F_exp))
print("Matriz de distancias ji-cuadrado entre filas:")
print(dist_ji)
```

{
  "hash": "b5773fff60cdeba56aa00c744016f3c9",
  "result": {
    "engine": "knitr",
    "markdown": "# Key Players and Topics\n\n# Non-Parametric Correlation {.unnumbered}\n\nCorrelation measures the strength and direction of association between two variables. While Pearson's correlation requires a linear relationship and normally distributed data, \\emph{Spearman's rank correlation} and \\emph{Kendall's tau} are \\emph{non-parametric} measures, making them ideal for analyzing data that may not be linear or normally distributed.\n\n## Spearman's Rank Correlation\n\nSpearman's correlation coefficient $\\rho$ is based on \\emph{ranked} data. For two variables $X$ and $Y$, we replace each observation by its rank.\n\n$\\rho = 1 - \\frac{6 \\sum d^2}{n(n^2 - 1)}$\n\nto compute Spearman's correlation coefficient $\\rho$. This measure is based on ranked data. For two variables $X$ and $Y$, we first replace each observation by its rank. Once the data are ranked, Spearman's $\\rho$ is computed similarly to Pearson's correlation, but using the ranks instead of the raw values:\n\n$\\rho = \\frac{\\sum_{i=1}^{n}(r_{X_i} - \\bar{r}_{X})(r_{Y_i} - \\bar{r}_{Y})}{\\sqrt{\\sum_{i=1}^{n}(r_{X_i} - \\bar{r}_{X})^2}\\sqrt{\\sum_{i=1}^{n}(r_{Y_i} - \\bar{r}_{Y})^2}}$\n\nwhere $r_{X_i}$ is the rank of the $i$-th observation of $X$, $r_{Y_i}$ is the rank of the $i$-th observation of $Y$, and $\\bar{r}_{X}$ and $\\bar{r}_{Y}$ are the mean ranks of $X$ and $Y$, respectively.\n\nSpearman's $\\rho$ is then computed similarly to Pearson's correlation but on these ranks:\n\n$\\rho = \\frac{\\sum_{i=1}^{n}(r_{X_i} - \\bar{r}_{X})(r_{Y_i} - \\bar{r}_{Y})}{\\sqrt{\\sum_{i=1}^{n}(r_{X_i} - \\bar{r}_{X})^2}\\sqrt{\\sum_{i=1}^{n}(r_{Y_i} - \\bar{r}_{Y})^2}}$\n\nwhere $r_{X_i}$ is the rank of the $i$-th observation of $X$, and $r_{Y_i}$ is the rank of the $i$-th observation of $Y$.\n\n-   A value of $\\rho$ close to 1 indicates a strong positive monotonic relationship (as one variable increases, so does the other), while a value close to -1 indicates a strong negative monotonic relationship. A value around 0 suggests little or no monotonic association.\n\n-   This method is robust to non-normality and outliers since it relies on the order (ranks) rather than the actual values.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(ggplot2)\n\n# 1. Create a small dataset\nx <- c(23, 42, 35, 44, 29)\ny <- c(52, 31, 14, 23, 45)\ndata <- data.frame(x = x, y = y)\n\n# 2. Compute the ranks for each variable\ndata$rank_x <- rank(data$x)  # For x: 1,2,3,4,5 (already sorted)\ncat('r_X = ', data$rank_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr_X =  1 4 3 5 2\n```\n\n\n:::\n\n```{.r .cell-code}\ndata$rank_y <- rank(data$y)  # For y: gives ranks corresponding to [2,1,4,3,5]\ncat('r_X = ', data$rank_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nr_X =  5 3 1 2 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Compute differences between ranks and their squares\ndata$d <- data$rank_x - data$rank_y\ndata$d2 <- data$d^2\n\n# Sum of squared differences\nsum_d2 <- sum(data$d2)\n\n# Number of observations\nn <- nrow(data)\n\n# 4. Calculate Spearman's correlation using the formula:\nspearman_rho <- 1 - (6 * sum_d2) / (n * (n^2 - 1))\n\n# Print computed Spearman correlation\nprint(paste(\"Spearman correlation (rho):\", round(spearman_rho, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Spearman correlation (rho): -0.7\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Expected output: 0.8\n\n# 5. Visualize the data using ggplot2\n\n# Plot 1: Original Data Scatter Plot with a Linear Fit\np1 <- ggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  ggtitle(\"Scatter Plot of Original Data\") +\n  xlab(\"x\") + ylab(\"y\") +\n  theme_minimal()\n\n# Plot 2: Scatter Plot of Ranks\np2 <- ggplot(data, aes(x = rank_x, y = rank_y)) +\n  geom_point(color = \"red\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  ggtitle(\"Scatter Plot of Ranks\") +\n  xlab(\"Rank of x\") + ylab(\"Rank of y\") +\n  theme_minimal()\n\n# Display the plots side by side\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](2-Key-Players-and-Topics_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Optionally, print the data frame to show ranks and differences\nprint(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x  y rank_x rank_y  d d2\n1 23 52      1      5 -4 16\n2 42 31      4      3  1  1\n3 35 14      3      1  2  4\n4 44 23      5      2  3  9\n5 29 45      2      4 -2  4\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndelitos_data <- delitos_data %>% \n  select(-sum_24TR, -sum_24SE, -sum_24SS)\n\ndelitos_data %>%\n  st_drop_geometry() %>%\n  select(contains('24')) %>%\n  cor(., method = \"spearman\", use = \"complete.obs\") %>%\n  round(., 3) %>% \n  print(.) %>%\n  corrplot(., method = \"color\", title = \"Spearman Correlation\", mar=c(0,0,1,0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          sum_24HOM sum_24LP sum_24VI sum_24DS sum_24HP sum_24HR sum_24HC\nsum_24HOM     1.000    0.070    0.049    0.045    0.054    0.013    0.039\nsum_24LP      0.070    1.000    0.188    0.157    0.228    0.091    0.136\nsum_24VI      0.049    0.188    1.000    0.162    0.205    0.102    0.088\nsum_24DS      0.045    0.157    0.162    1.000    0.166    0.075    0.101\nsum_24HP      0.054    0.228    0.205    0.166    1.000    0.166    0.262\nsum_24HR      0.013    0.091    0.102    0.075    0.166    1.000    0.088\nsum_24HC      0.039    0.136    0.088    0.101    0.262    0.088    1.000\nsum_24HA      0.039    0.096    0.081    0.064    0.142    0.061    0.069\nsum_24HM      0.045    0.112    0.118    0.095    0.142    0.073    0.079\nsum_24EX      0.027    0.087    0.086    0.084    0.137    0.067    0.095\n          sum_24HA sum_24HM sum_24EX\nsum_24HOM    0.039    0.045    0.027\nsum_24LP     0.096    0.112    0.087\nsum_24VI     0.081    0.118    0.086\nsum_24DS     0.064    0.095    0.084\nsum_24HP     0.142    0.142    0.137\nsum_24HR     0.061    0.073    0.067\nsum_24HC     0.069    0.079    0.095\nsum_24HA     1.000    0.104    0.067\nsum_24HM     0.104    1.000    0.062\nsum_24EX     0.067    0.062    1.000\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](2-Key-Players-and-Topics_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\nBoth Spearman's $\\rho$ captures the \\emph{monotonic} relationship between two variables. They are more robust to outliers and non-linear relationships than Pearson's correlation. In the context of areal data (e.g., crime rates, population density across polygons), these measures can reveal how variables co-vary without assuming linearity or normality.\n\n# Spatial Neighborhood Matrices {.unnumbered}\n\nThis section is based on *Spatial statistics for data science theory and practice with R*. See [@Moraga2023].\n\n## Neighbors Based on Contiguity\n\n-   Queen Contiguity: Two polygons are considered neighbors if they share any common point (i.e., an edge or a vertex). Mathematically, if polygons $p_i$ and $p_j$ touch at any point, then $A\\_{ij} = 1$.\n-   Rook Contiguity: Two polygons are neighbors only if they share a common edge. That is, if polygons $p_i$ and $p_j$ share a boundary segment, then $A\\_{ij} = 1$; merely touching at a corner does not count.\n\n[![Neighbors based on contiguity. Area of interest is represented in black and its neighbors in gray.](images/arealdata-orderk-1.png){fig-align=\"center\"}](Neighbors%20based%20on%20contiguity.%20Area%20of%20interest%20is%20represented%20in%20black%20and%20its%20neighbors%20in%20gray.)\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a spatial neighbors list using Queen contiguity\n# (i.e., polygons are considered neighbors if they share any point: edge or vertex)\nnb <- spdep::poly2nb(delitos_data, queen = TRUE)\nhead(nb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 14209 37157\n\n[[2]]\n[1]  2541  2852  3535 18019 23777 37158\n\n[[3]]\n[1] 0\n\n[[4]]\n[1] 37528\n\n[[5]]\n[1] 0\n\n[[6]]\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Replace invalid neighbor entries (i.e., [1] 0) with empty integer vectors\n# This ensures compatibility with functions that expect valid neighbor lists only\nnb_0 <- lapply(nb, function(x) if(length(x)==1 && x==0) integer(0) else x)\n\n# Polygons with neighbors\ntable(sapply(nb_0, length))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n38443  3992  1057   407   172    95    59    38    19    10     7     7     5 \n   13    14    15    16    17    18    29 \n    4     1     3     2     1     2     1 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Neighbors of Order k Based on Contiguity\n# Neighbors of second order\nnblags <- spdep::nblag(neighbours = nb, maxlag = 2)\n\n# Combine neighbors of all orders up to the specified lag (in this case, up to order 2)\n# This creates a cumulative neighbor list including first- and second-order neighbors\nnblagsc <- spdep::nblag_cumul(nblags)\ntable(sapply(nblagsc, length))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n39843  1068   762   632   461   397   293   193   137    93    85    67    61 \n   14    15    16    17    18    19    20    21    24    29    30    32    33 \n   31    52    47    24    37     5     3     1     1    17     7     1     4 \n   42    46    48 \n    1     1     1 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## Neighbors Based on k Nearest Neighbors\n\n-   K-Nearest Neighbors: For each polygon, the ( k ) nearest neighbors are identified based on a distance threshold.\n-   Distance Threshold: The distance threshold can be defined as a fixed value or as a function of the average distance between polygons.\n\n\\textbf{k-Nearest Neighbors (kNN)} is a method that defines neighbors based on distance rather than contiguity. For each spatial unit $p_i$, the $k$ closest units (according to Euclidean distance or other metric) are selected as neighbors.\n\nFormally, let $D(p_i, p_j)$ be the distance between polygons $p_i$ and $p_j$. Then, the neighbor set $N_k(p_i)$ is defined as:\n\n$N_k(p_i) = p_j$ : $p_j$ is among the $k$ nearest polygons to $p_i$.\n\nThis ensures that each polygon has exactly $k$ neighbors, which is useful when spatial units are irregular or disconnected.\n\n[![We define spatial k-nearest neighbour problem as finding k observations from a set of candidates C that are the most similar to the given a landmark L\\[i\\], where the similarity is defined by a distance function d(L\\[i\\], S\\[j\\]) = st_distance(L\\[i\\], S\\[j\\])](images/nearest_neighbour.png){fig-align=\"center\"}](https://databrickslabs.github.io/mosaic/models/spatial-knn.html)\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute centroids of the polygons\ncoo <- st_centroid(delitos_data)\n\n# Create a neighbor list where each polygon (based on its centroid `coo`) is connected \n# to its 3 nearest neighbors using k-nearest neighbors (k = 3)\nnb <- knn2nb(knearneigh(coo, k = 3)) # k number nearest neighbors\n\n# Polygons with neighbors\ntable(sapply(nb, length))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    3 \n44325 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Subset data to the first 10 polygons\ndelitos_data_10 <- delitos_data[1:100, ]\n\n# Recompute neighbor list for these 10 polygons to avoid index mismatches\nnb_10 <- knn2nb(knearneigh(st_centroid(delitos_data_10), k = 3))\n\n# Compute centroids for the 10 polygons\ncoords_10 <- st_coordinates(st_centroid(delitos_data_10))\n\n# Plot the first 10 polygons and overlay neighbor connections in red\nplot(st_geometry(delitos_data_10), border = \"lightgray\", main = \"First Polygons with 3 Nearest Neighbors\")\nplot.nb(nb_10, coords_10, add = TRUE, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](2-Key-Players-and-Topics_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n## Neighbors Based on Distance\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a neighbor list using distance-based contiguity:\n# Polygons are considered neighbors if their centroids are within 0.4 units (e.g., degrees) apart\nnb <- dnearneigh(x = st_centroid(delitos_data), d1 = 0, d2 = 0.4)\n\n# Polygons with neighbors\nhist(sapply(nb, length))\n```\n\n::: {.cell-output-display}\n![](2-Key-Players-and-Topics_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Subset data to the first 10 polygons\ndelitos_data_10 <- delitos_data[1:100, ]\n\n# Recompute neighbor list for these 10 polygons to avoid index mismatches\nnb_10 <- dnearneigh(x = st_centroid(delitos_data_10), d1 = 0, d2 = 0.4)\n\n# Compute centroids for the 10 polygons\ncoords_10 <- st_coordinates(st_centroid(delitos_data_10))\n\n# Plot the first 10 polygons and overlay neighbor connections in red\nplot(st_geometry(delitos_data_10), border = \"lightgray\", main = \"First Polygons with 3 Nearest Neighbors\")\nplot.nb(nb_10, coords_10, add = TRUE, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](2-Key-Players-and-Topics_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\nDetermining an Appropriate Upper Distance Bound: To ensure that each area in a spatial dataset has at least (k) neighbors, we can determine an appropriate upper distance bound by first computing the (k) nearest neighbors for each area. For example, using the Queen contiguity method, one may use the \\texttt{spdep::knearneigh()} function with (k=1) to obtain the nearest neighbor for each polygon. This yields a matrix of neighbor IDs, which is then converted into a neighbor list (of class \\texttt{nb}) via \\texttt{knn2nb()}. Next, the \\texttt{spdep::nbdists()} function computes the distances along the links between each area and its neighbor. By summarizing these distances (e.g., using \\texttt{summary(unlist(dist1))}), we can observe the range of distances.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute k-nearest neighbors: for each polygon centroid, find its 1 nearest neighbor (k = 1)\nnb1 <- knn2nb(knearneigh(coo, k = 1))\n\n# Calculate the Euclidean distances between each polygon and its nearest neighbor\ndist1 <- nbdists(nb1, coo)\n\n# Summarize all distances to understand the minimum, maximum, and quartiles\nsummary(unlist(dist1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.001377 0.031220 0.044383 0.053259 0.063116 1.196872 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a distance-based neighbor list: polygons whose centroids are within [0, 1.2] units are considered neighbors\nnb <- dnearneigh(x = st_centroid(delitos_data), d1 = 0, d2 = 1.2)\n\n# Polygons with neighbors\nhist(sapply(nb, length))\n```\n\n::: {.cell-output-display}\n![](2-Key-Players-and-Topics_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n## Neighborhood Matrices\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Spatial weights matrix using Queen contiguity (binary weights)\n# 'queen = TRUE' considers shared edges OR vertices as neighbors\nnb <- poly2nb(delitos_data, queen = TRUE)\n\n# Convert the neighbor list to a spatial weights list object\n# 'style = \"W\"' row-standardizes the weights (sums to 1)\n# 'zero.policy = TRUE' avoids errors when some polygons have no neighbors\nnbw <- spdep::nb2listw(nb, style = \"W\", zero.policy = TRUE)\n\n# Display the first 10 rows of spatial weights (for the first 10 polygons)\nnbw$weights[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.5 0.5\n\n[[2]]\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\n[[3]]\nNULL\n\n[[4]]\n[1] 1\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n[[7]]\n[1] 0.5 0.5\n\n[[8]]\n[1] 1\n\n[[9]]\nNULL\n\n[[10]]\n[1] 0.5 0.5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Spatial weights matrix based on inverse distance values\n# Compute centroids of polygons\ncoo <- st_centroid(delitos_data)\n\n# Use Queen contiguity again to define neighbors\nnb <- poly2nb(delitos_data, queen = TRUE)\n\n# Compute distances between neighbors based on their centroids\ndists <- nbdists(nb, coo)\n\n# Create inverse distance weights (1/distance) for each pair of neighbors\nids <- lapply(dists, function(x){1/x})\n\n# Create a listw object using binary style (\"B\" = no standardization)\nnbw <- nb2listw(nb, glist = ids, style = \"B\", zero.policy = TRUE)\n\n# Display the first 10 inverse-distance-based weights\nnbw$weights[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 10.48386 12.05151\n\n[[2]]\n[1] 18.292399  8.350533 24.123987  8.824175 22.054259 11.962069\n\n[[3]]\nNULL\n\n[[4]]\n[1] 1.813293\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n[[7]]\n[1] 0.4525287 0.4234258\n\n[[8]]\n[1] 0.8290768\n\n[[9]]\nNULL\n\n[[10]]\n[1] 0.2181132 0.2194360\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n# Spatial autocorrelation\n\n# Correspondance Analysis {.unnumbered}\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "2-Key-Players-and-Topics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
# Appendix 1 {.appendix, .unnumbered}

12/05/2025

### Andres Cervantes \[76\]

#### Maestría en Matemáticas Aplicadas y Ciencias de la Computación

#### Mayo 2025

Diagonal Values:

The entries on the diagonal (e.g., (42.8134) for sum_24HOM, (4.905) for sum_24LP, etc.) represent the conditional variance of each variable. These values ​​indicate how much each variable varies relative to its relationship with all other variables in the model. A higher value implies that the variable has greater conditional variability.

For the assignment we selected three departments: Antioquia \[05\], Valle del Cauca \[76\] and Atlántico \[08\]; and ran the statistic analysis for each individually. From the data summary, for most crime variables, from the first to the third quantile values are 0, whereas the maximum value usually is a bigger number, which allows us to inquire that crime is focused in some polygons.

We decided to focus on the problem of personal injury. The box plot and quantile analysis show that, like other crime data, homicides in Atlántico are concentrated in a few industrial areas, and 10% of these industrial areas accounted for all homicides in 2024.

```{r}
#| include: false
source('setup.R')

# Load the dataset
delitos_data <- st_read("data/spatial/crime_spatial_course.gpkg")
delitos_data <- delitos_data[delitos_data$dpto_ccdgo == '76', ]
```

```{r}
#PorAndrés Cervantes
#------------------------------------------------
# delitos_data$sum_24HP es un vector.
# Tamaño del vector:
#---largo <- length(delitos_data$sum_24HP)
#---cat("Cantidad de columnas:", largo, "\n")
#------------------------------------------------
#---media <- mean(delitos_data$sum_24HP, na.rm = TRUE) # na.rm = TRUE para ignorar valores NA
#---print(media)
#------------------------------------------------
#Descargar Dataset en Excel
#---install.packages("openxlsx")
#---library(openxlsx)
# Suponiendo que 'delitos_data$sum_24HP' es un vector o parte de un data frame
#---write.xlsx(data.frame(sum_24HP = delitos_data$sum_24HP), "delitos_data_sum_24HP.xlsx")
#------------------------------------------------
#Ruta de trabajo actual
###-getwd()
#------------------------------------------------
```

```{r}
#| echo: true
#| message: false
#| warning: false
# working directory
#setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# packages
list_packages = c('readxl', 'dplyr', 'moments', 'tidyr', 'tibble', 'gt', 'ggplot2', 'fmsb', 'car', 'reshape2', 'knitr', 'gridExtra', 'ggExtra', 'sf', 'leaflet', 'igraph', 'ggraph', 'tidygraph', 'spdep')
new.packages = list_packages[!(list_packages %in% installed.packages()[,"Package"])]
if (length(new.packages)) {
  install.packages(new.packages)
}
for (package in list_packages){
  library(package, character.only = T)
}

# Load the dataset
delitos_data <- st_read("data/spatial/crime_spatial_course.gpkg")
delitos_data <- delitos_data[delitos_data$dpto_ccdgo == c("76" ), ] # Valle del Cauca

dim(delitos_data)
summary(delitos_data)

# interactive polygons location
leaflet(delitos_data) %>%
  addTiles() %>%  # Base map
  addPolygons(color = "steelblue", weight = 1, fillOpacity = 0.5)

# quantile
quantile(delitos_data$sum_24HP, probs = seq(0, 1, 0.1), na.rm = TRUE)

#boxplot
boxplot(delitos_data$sum_24HP, main = "Boxplot of Homicides in Valle del Cauca", horizontal = TRUE)
```

```{r}
> # quantile
> quantile(delitos_data$sum_24HP, probs = seq(0, 1, 0.1), na.rm = TRUE)
  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% 
   0    0    0    0    0    0    0    0    0    1   80
```

Here we have the data divided into 10 quantiles (1 decile). That is, all observations have been taken and divided into 10 equal parts. Remember that a data set can be divided into quantiles. Quantile is a generic term for grouping: quartiles, quintiles, deciles, percentiles. It is clear that up to the 80th percentile, the result of each observation is 0. That is, no crimes were recorded in 80% of the total set of observations. Then, at the 90th percentile, it can be determined that there was at least one recorded crime. Finally, in the last decile, or rather, in the last 10% of the data, the maximum number of recorded crimes was 80.

## Skewness

```{r}
#| echo: true
#| message: false
#| warning: false

# step by step
n <- length(delitos_data$sum_24HP) 
mean_x <- mean(delitos_data$sum_24HP)
dato = mean_x <- mean(delitos_data$sum_24HP)
sd_x <- sd(delitos_data$sum_24HP)  # Uses (n-1) denominator
z_scores <- (delitos_data$sum_24HP - mean_x) / sd_x
z_cubed <- z_scores^3
sum_cubed <- sum(z_cubed)
skewness <- (n / ((n - 1) * (n - 2))) * sum_cubed
paste0('sum_24HP: ', skewness)

# function
skewness(delitos_data$sum_24HP, na.rm = TRUE)

# skewness
delitos_data %>%
  st_drop_geometry() %>%
  select(dpto_ccdgo, contains('24')) %>%
  group_by(dpto_ccdgo) %>%
  summarise(across(everything(), ~ skewness(.x, na.rm = TRUE))) %>%
  pivot_longer(
    cols = -dpto_ccdgo,
    names_to = "Crime Type",
    values_to = "Skewness"
  ) %>%
  mutate(Skewness = round(Skewness, 2)) %>%
  pivot_wider(
    names_from = dpto_ccdgo,
    values_from = Skewness
  ) %>%
  gt()

```

Robberies in **Valle del Cauca** data have a skewness of *23.45458*.\
This is an extremely high value, implying that the data distribution is strongly skewed to the right. This high level of skewness indicates that there are many outliers (extreme values) at the top of the distribution. The shape of the distribution under these conditions would not be symmetrical. Instead, it would be "right-skewed," meaning that most of the data are clustered on the left side of the graph, with a long tail extending to the right.

```{r}
library(ggplot2)
library(moments)
library(dplyr)

# Calcular la asimetría
skewness_value <- skewness(delitos_data$sum_24HP, na.rm = TRUE)
print(paste("Asimetría:", skewness_value))

# Crear el gráfico de frecuencia
ggplot(delitos_data, aes(x = sum_24HP)) +
  geom_bar(stat = "count", fill = "lightblue", color = "black", alpha = 0.7) +
  labs(title = "Frecuencia de Delitos (sum_24HP)",
       x = "Número de delitos (sum_24HP)",
       y = "Frecuencia") +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0, 80, by = 5))  # Muestra valores en el eje X desde 0 hasta 80, en incrementos de 5
```

------------------------------------------------------------------------

> paste0('sum_24HP: ', skewness) \[1\] "sum_24HP: 108.920265681" \> \> \# function \> skewness(delitos_data\$sum_24HP, na.rm = TRUE) \[1\] 108.917

```{r}
# Calcular la asimetría
skewness_value <- skewness(delitos_data$sum_24HP, na.rm = TRUE)
print(paste("Asimetría:", skewness_value))

# Crear el gráfico
ggplot(delitos_data, aes(x = sum_24HP)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_density(color = "red", linewidth = 1) +  # Cambiado a linewidth
  geom_vline(aes(xintercept = mean(sum_24HP, na.rm = TRUE)), color = "green", linetype = "dashed", linewidth = 1) +  # Cambiado a linewidth
  geom_vline(aes(xintercept = median(sum_24HP, na.rm = TRUE)), color = "blue", linetype = "dashed", linewidth = 1) +  # Cambiado a linewidth
  labs(title = "Histograma de sum_24HP con Densidad",
       x = "Número de delitos (sum_24HP)",
       y = "Densidad") +
  theme_minimal()

```

## Kurtosis

```{r}
#| echo: true
#| message: false
#| warning: false

# step by step
z_fourth <- z_scores^4
sum_fourth <- sum(z_fourth)
kurtosis <- ((n * (n + 1)) / ((n - 1) * (n - 2) * (n - 3))) * sum_fourth - (3 * (n - 1)^2) / ((n - 2) * (n - 3))
print(kurtosis)

# function
kurtosis(delitos_data$sum_24HP, na.rm = TRUE)

# Kurtosis
delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  summarise(across(everything(), ~ kurtosis(.x, na.rm = TRUE))) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Kurtosis = V1) %>%
  gt()

```

For the case of **Valle del Cauca**: we have a kurtosis of 1056, indicates that the distribution not only has heavy tails, but also has a very sharp peak. This behavior is common in distributions where there are significant outliers. This will be a leptokurtic distribution.

## Coefficient of Variation

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute statistics
mean_val <- mean(delitos_data$sum_24HP, na.rm = TRUE)
print(mean_val)
std_dev <- sd(delitos_data$sum_24HP, na.rm = TRUE)
print(std_dev)

# Compute the range for first standard deviation
lower_bound <- mean_val - std_dev
upper_bound <- mean_val + std_dev
paste0('lower_bound: ', round(lower_bound, 2), ' - upper_bound: ', round(upper_bound, 2))

# Count the number of points within 1 standard deviation
within_1sd <- sum(delitos_data$sum_24HP >= lower_bound & delitos_data$sum_24HP <= upper_bound, na.rm = TRUE)
percentage_1sd <- (within_1sd / nrow(delitos_data)) * 100
paste0('within_1sd: ', round(within_1sd, 2), ' - percentage_1sd: ', round(percentage_1sd, 2))

# Create histogram
ggplot(delitos_data, aes(x = sum_24HP)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.5, color = "black") +
  
  # Add vertical lines for mean, median, and 1st SD
  geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1.2) +
  #geom_vline(aes(xintercept = median_val), color = "green", linetype = "dashed", size = 1.2) +
  geom_vline(aes(xintercept = lower_bound), color = "purple", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = upper_bound), color = "purple", linetype = "dashed", size = 1) +
  
  # Labels and title
  labs(title = "Histogram of AUTOMOTORES with Mean, and 1SD Range",
       x = "AUTOMOTORES Values", y = "Frequency") +
  
  # Add annotation for 1SD range
  annotate("text", x = mean_val, y = 10, 
           label = paste(round(percentage_1sd, 1), "1SD", sep = ""), 
           color = "black", size = 5, hjust = 0.5, vjust = -1) +
  
  theme_minimal()

# cv
paste0('cv: ', round(std_dev / mean_val * 100), 2)

# variation
delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  summarise(
    across(
      everything(),
      ~ ifelse(mean(.x, na.rm = TRUE) != 0, 
               sd(.x, na.rm = TRUE) / mean(.x, na.rm = TRUE), 
               NA),  # Compute CV safely
      .names = "{col}"
    )
  ) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Crime Type") %>%
  mutate(V1 = round(V1, 2)) %>%
  rename(Variation = V1) %>%
  gt()
```

For the case of **Valle del Cauca**: this value (4392) indicates that the standard deviation is more than 43 times larger than the mean (0.2616484). Such a high CV suggests that the values ​​in your data set are extremely dispersed relative to the mean. A CV of 4.39 in sum_24HP (or 439%) also indicates that the variability of this data is high relative to its mean. However, this number, although significant, is considerably lower compared to the overall coefficient of variation. We are definitely facing a high dispersion in the data set.

## Median Absolute Deviation MAD and MAD/median

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute statistics
median_val <- median(delitos_data$sum_24HP, na.rm = TRUE)
print(median_val)
mad_val <- mad(delitos_data$sum_24HP, na.rm = TRUE)  # Compute MAD
print(mad_val)

# Compute the range for first standard deviation
lower_bound <- median_val - mad_val
upper_bound <- median_val + mad_val
paste0('lower_bound: ', round(lower_bound, 2), ' - upper_bound: ', round(upper_bound, 2))

# Count the number of points within 1 MAD
within_1mad <- sum(delitos_data$sum_24HP >= lower_bound & delitos_data$sum_24HP <= upper_bound, na.rm = TRUE)
percentage_1mad <- (within_1mad / nrow(delitos_data)) * 100
paste0('within_1mad: ', round(within_1mad, 2), ' - percentage_1mad: ', round(percentage_1mad, 2))

# Create histogram
ggplot(delitos_data, aes(x = sum_24HP)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.5, color = "black") +
  
  # Add vertical lines for mean, median, and 1st SD
  #geom_vline(aes(xintercept = mean_val), color = "red", linetype = "dashed", size = 1.2) +
  geom_vline(aes(xintercept = median_val), color = "green", linetype = "dashed", size = 1.2) +
  geom_vline(aes(xintercept = lower_bound), color = "purple", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = upper_bound), color = "purple", linetype = "dashed", size = 1) +
  
  # Labels and title
  labs(title = "Histogram of AUTOMOTORES with Median, and 1MAD Range",
       x = "AUTOMOTORES Values", y = "Frequency") +
  
  # Add annotation for 1SD range
  annotate("text", x = median_val, y = 10, 
           label = paste(within_1mad, "points (", round(percentage_1mad, 1), "1MAD", sep = ""), 
           color = "black", size = 5, hjust = 0.5, vjust = -1) +
  
  theme_minimal()

# MAD/Median
paste0('MAD/Median: ', round(mad_val / median_val * 100), 2)
```

**Valle del Cauca:** The formula to calculate the MAD of the data is as follows.

Here we have a dataset of 45,328 values. They're distributed in 2 groups:

Zero values = 38484.

Non-zero values = 6844.

1st: x = calculate the median for all dataset. R/ 0.

2nd: y(array) = calculate absolute deviations. \[median - each dataset value\].

3th: z = calculate the median of the absolute deviations. \[median (all absolute deviations obtained)\].

So, MAD = \[z\] = 0 and MAD/median = \[(z / x)\*100\] then 0 as well. If you ask why multiply by 100, it is because we want to obtain a value as a percentage.

A MAD of 0 indicates that there is no dispersion in the data around the median. In other words, all the values ​​in your data set are equal to the median (in this case, 0). There is no variance in the data, meaning there are no absolute deviations from the median.

Equality to zero in both cases (MAD and median) indicates that there is a lack of variability in the data set and may be an indicator that all elements are homogeneous.

## Covariance Matrix

```{r}
#| echo: true
#| message: false
#| warning: false
delitos_data %>%
  st_drop_geometry() %>%
  select(contains("24")) %>%
  cov() %>%
  round(2) %>%
  knitr::kable(digits = 2, caption = "Covariance Matrix")
```

**Valle del Cauca**: this matrix is ​​constructed using data from the year 2024. Of all the crimes, the central variable is "sum_24HP", which, as we saw before, in this data set has a variance of 1.32 and a mean of 0.2616.

```{r}
Covariance Matrix for sum_24HP	
sum_24HP	Value
sum_24HOM	0
sum_24LP	0.13
sum_24VI	0.18
sum_24DS	0.05
sum_24HP	1.32
sum_24HR	0.02
sum_24HC	0.18
sum_24HA	0.03
sum_24HM	0.06
sum_24SS	0
sum_24SE	0
sum_24EX	0.02
sum_24TR	0

```

**Prompt :** let's talk about Covariance Matrix.

Is this a multivariate calculation?

Yes, it is used to describe the joint variability of a set of variables. Specifically, it measures how two variables change relative to each other.

It contains the covariances between all combinations of a set of variables. Each element (C\_{ij}) of the matrix represents the covariance between variable (i) and variable (j).

**Interpreting the Elements:**

If (C\_{ij}) is positive, it indicates that when one of the variables increases, the other tends to increase as well.

If (C\_{ij}) is negative, it indicates that when one of the variables increases, the other tends to decrease.

If (C\_{ij}) is zero, it suggests that there is no linear relationship between the two variables.

For us in this case is important to evaluate "sum_24HP" against others. Here we can see how strong is its covariance and how two variables change relative to each other.

------------------------------------------------------------------------

## Covariance Matrix of Log-Transformed Data

```{r}
#| echo: true
#| message: false
#| warning: false

# Define the dataset
x <- delitos_data$sum_24HP

# 1. Compute Raw Arithmetic Mean
arithmetic_mean <- mean(x)
print(arithmetic_mean)

# 2. Compute Log-Mean (Multiplicative Center)
log_x <- log(x + 1)  # Take logarithm of values
head(log_x)
log_mean <- mean(log_x)  # Compute mean in log-space
print(log_mean)
log_mean_exp <- exp(log_mean)  # Convert back to original scale
print(log_mean_exp)

# Create the comparison table
comparison_table <- data.frame(
  Index = seq_along(x),  # Just an index for x-axis
  Original_Value = x,
  Log_Value = log_x
)

p1 <- ggplot(comparison_table, aes(x = Original_Value, y = Log_Value)) +
  geom_line(color = "gray70", size = 0.7, alpha = 0.5) +  # Thin line connecting points
  geom_point(alpha = 0.7, color = "blue") +  # Scatter points with transparency
  labs(
    title = "Scatter Plot: Original vs. Log-Transformed Values",
    x = "Original Values",
    y = "Log-Transformed Values"
  ) +
  theme_minimal()

# Add marginal histogram
ggMarginal(
  p1,
  type = "histogram",         # Add marginal histograms
  bins = 40,                  # Number of bins for the histogram
  margins = "both",           # Add histogram to both x and y margins
  size = 5,                   # Size of the histograms relative to the scatter plot
  fill = "gray",              # Fill color for the histogram
  color = "black",            # Outline color for the histogram
  alpha = 0.5                 # Transparency
)
```

```{r}
#| include: false
# Store values for inline Quarto text
log_values <- paste(round(head(comparison_table$Log_Value), 2), collapse = ", ")
original_values <- paste(head(comparison_table$Original_Value), collapse = ", ")
```

**Valle del Cauca:**

**Prompt** #1:

What is Covariance Matrix of Log-Transformed Data and what is its relationship with Covariance Matrix?

#### **Log-Transformación**

-   It's a technique that converts data by applying the logarithmic function. This can help stabilize variance and make the data follow a distribution closer to normal, especially if the original data are highly skewed or have a wide range of values.

#### **Covariance Matrix of Log-Transformed Data**

-   **Calculation**: to obtain the covariance matrix of log-transformed data, you first apply the logarithmic transformation to your data and then calculate the covariance matrix of the transformed data. This is done as follows:

    1.  **Transform Data**: applies the logarithmic function (can be log base 10 or natural log, depending on the context) to each element of the data set.

    2.  **Calculate the Covariance Matrix**: use the covariance formula mentioned above to calculate the matrix, but now using the log-transformed values.

#### **Relatioship between Covariance Matrix and Covariance Matrix of Log-Transformed Data**

-   The covariance matrix of the original data and that of the log-transformed data can differ significantly, especially if the original data are skewed. Log-transformation can make these relationships more evident or interpretable, especially in cases where the relationship between variables is nonlinear.

-   The main difference is that the covariance matrix of log-transformed data reflects the relationships and variability of the data in a logarithmic space, which may be more suitable for statistical models that assume normality.

**Prompt** #2:

Again, we're talking about the relationship between the covariance matrix and the covariance matrix of log-transformed data. It's not clear to me why applying a logarithm is convenient, what does it change? How does it benefit me? Can you create an example with fictitious data? Also, what is heteroskedasticity?

#### Example with Fictitious Data

```{r}
# Crear datos de ejemplo
set.seed(1)
años <- 1:10
# Ejemplo de ganancias (en miles de dólares)
ganancias <- c(1, 2, 1.5, 3, 50, 100, 200, 400, 800, 2000) 
# Generar una segunda variable, ingresos
ingresos <- c(5, 10, 15, 20, 60, 110, 220, 400, 900, 2500)  # otra variable

# Crear un marco de datos
datos <- data.frame(años, ganancias, ingresos)

# Visualizar los datos originales
print(datos)

# Calcular la matriz de covarianza original
cov_original <- cov(datos[, c("ganancias", "ingresos")])
print("Covarianza original:")
print(cov_original)

# Aplicar logaritmo a las ganancias y a los ingresos
datos$log_ganancias <- log(datos$ganancias)
datos$log_ingresos <- log(datos$ingresos)

# Visualizar los datos transformados
print(datos)

# Calcular la matriz de covarianza log-transformada
cov_log <- cov(datos[, c("log_ganancias", "log_ingresos")])
print("Covarianza log-transformada:")
print(cov_log)
```

We have a data set representing a company's earnings in thousands of dollars over several years:

Analysis:

Original Data: The earnings have a wide range (from 1 to 2000), which could be causing the variance to be inconsistent (heteroskedasticity). The covariance could make interpretation difficult due to these biases.

Log-transformed Data: By applying the logarithm, we are compressing these values ​​to a more manageable range. The data will now be less skewed, and the variance will become more constant.

```{r}

años------ganancias------ingresos
1-------------1--------------5
2-------------2--------------10
3-------------1.5------------15
4-------------3--------------20
5-------------50-------------60
6-------------100------------110
7-------------200------------220
8-------------400------------400
9-------------800------------900
10-----------2000------------2500

[1] "Covarianza original:"           
           ganancias    ingresos 
ganancias  398548.4     492191.9 
ingresos   492191.9     609676.7

[1] "Covarianza log-transformada:"               
               log_ganancias  log_ingresos 
log_ganancias  8.121839       5.762011 
log_ingresos   5.762011       4.244732
```

```{r}
años	ganancias	ingresos	log_ganancias	log_ingresos
1	    1		      5		      0		          1.609438
2	    2		      10		    0.6931472	    2.302585
3	    1.5		    15		    0.4054651	    2.70805
4	    3		      20		    1.0986123	    2.995732
5	    50		    60		    3.912023	    4.094345
6	    100		    110		    4.6051702	    4.70048
7	    200		    220		    5.2983174	    5.393628
8	    400		    400		    5.9914645	    5.991465
9	    800		    900		    6.6846117	    6.802395
10	  2000		  2500		  7.6009025	    7.824046
```

It is important to clarify that the natural logarithm has been used to make these transformations.

#### Heteroscedasticity

Refers to a condition in which the error variance of a regression model is not constant across all levels of the independent variable.

### Benefits of Logarithmic Transformation and Heteroskedasticity Resolution

By applying a logarithm:

**We reduce the impact of extreme values:** Extremely high data values ​​have less influence.

**We stabilize the variance:** Residuals become more consistent throughout the prediction.

**We facilitate interpretation:** Relationships are often expressed in percentage terms, which can be more intuitive.

The logarithmic transformation is a powerful tool in statistical and data analysis, especially when working with skewed data or in situations where variability is not constant (heteroskedasticity).

#### Returning to results for Valle del Cauca

The arithmetic mean of the data set is: **0.261648**

**First 6 logarithmic transformations (with +1 to avoid log(0)): \[1\] 0 0 0 0 0 0**

This may indicate that many of the data in sum_24HP are zeros, which could be a reflection of the nature of your data set (for example, if sum_24HP corresponds to counts of events that do not occur regularly).

**The log mean (multiplicative center) is: 0.134861**

The calculated log mean is approximately (0.134861). This is a value in log space and suggests that, although most of the values ​​are zero, there are some higher values ​​influencing this mean.

**The log mean converted back to the original scale is: 1.144378**

The combined value (multiplicative center) converted back to the original scale is approximately (1.144378). This means that the typical (average) value of the data, when considering multiplicative variability, is about (1.144378).

### General Conclusions

Distribution and Validity of Results: The presence of many zeros in the data suggests that there may be a high concentration of data at the lower end (zero or near zero), which is correct when we look at the dataset. This can influence the interpretation of the arithmetic mean. The log mean may be more stable in such cases, as it handles extreme or skewed values ​​better.

Use of the Log Transformation: The log transformation was appropriate in this case, as it avoided the problem of errors when calculating the log of zero and provided a measure that takes into account the variation in the data.

```{r}
# Covariance Matrix (Log-Transformed) for sum_24HP
# Variable  Value
# sum_24HOM	0
# sum_24LP	0.02
# sum_24VI	0.02
# sum_24DS	0
# sum_24HP	0.12
# sum_24HR	0
# sum_24HC	0.01
# sum_24HA	0.01
# sum_24HM	0.01
# sum_24SS	0
# sum_24SE	0
# sum_24EX	0
# sum_24TR	0

```

------------------------------------------------------------------------

```{r}
#| echo: true
#| message: false
#| warning: false

#log transformed data
# Compute statistics for raw and log-transformed data
mean_raw <- mean(delitos_data$sum_24HP, na.rm = TRUE)
sd_raw <- sd(delitos_data$sum_24HP, na.rm = TRUE)
mad_raw <- mad(delitos_data$sum_24HP, na.rm = TRUE)

delitos_data_log <- delitos_data %>%
  #mutate(LOG_AUTOMOTORES = log(AUTOMOTORES + 1))
  mutate(LOG_AUTOMOTORES = log1p(sum_24HP))  # log1p(x) = log(1 + x) to handle zeros

mean_log <- mean(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)
sd_log <- sd(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)
mad_log <- mad(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)

# Compute statistics for raw and log-transformed data
data.frame(
  Measure = c("Mean", "Median", "Standard Deviation", "MAD"),
  Raw_Data = c(mean(delitos_data$sum_24HP, na.rm = TRUE),
               median(delitos_data$sum_24HP, na.rm = TRUE),
               sd(delitos_data$sum_24HP, na.rm = TRUE),
               mad(delitos_data$sum_24HP, na.rm = TRUE)),
  Log_Transformed_Data = c(mean(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE),
                           median(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE),
                           sd(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE),
                           mad(delitos_data_log$LOG_AUTOMOTORES, na.rm = TRUE)))

# Transform the data to a long format for ggplot
delitos_long <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Value")

# Create faceted histograms
ggplot(delitos_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Distributions of Crime Data",
    x = "Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 5)  # Reduce the font size of X-axis text
  )

# Transform the data to long format and apply log transformation
delitos_long_log <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  mutate(across(everything(), ~ log(.x), .names = "{col}")) %>%  # Log transform (log(x + 1) to avoid log(0))
  pivot_longer(cols = everything(), names_to = "Crime Type", values_to = "Log Value")

# Create faceted histograms for log-transformed values
ggplot(delitos_long_log, aes(x = `Log Value`)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ `Crime Type`, scales = "free") +  # Facet by crime type
  theme_minimal() +
  labs(
    title = "Log-Transformed Distributions of Crime Data",
    x = "Log Value",
    y = "Frequency"
  ) +
  theme(
    axis.text.x = element_text(size = 3)  # Reduce the font size of X-axis text
  )

# Covariance Matrix (Log-Transformed)
delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  mutate(across(everything(), ~ log(.x+1))) %>%  # Log-transform (+1 to handle zeros)
  cov() %>%
  round(2) %>%
  kable(digits = 2, caption = "Covariance Matrix (Log-Transformed)")
```

## Redundant Variables

Redundant variables provide little additional information due to high correlation with others, leading to multicollinearity in models.

```{r}
#| echo: true
#| message: false
#| warning: false

# Define the matrix A
matrix_a <- matrix(c(4, 2,
                     2, 3), nrow = 2, byrow = TRUE)
print("Matrix A:")
print(matrix_a)

# Compute the eigen decomposition using R's built-in eigen() function
eigen_builtin <- eigen(matrix_a)
print("Built-in eigen() values:")
print(eigen_builtin$values)
print("Built-in eigen() vectors:")
print(eigen_builtin$vectors)

# Multiply A by the matrix of eigenvectors:
# Each column of eigen_builtin$vectors is an eigenvector.
res <- matrix_a %*% eigen_builtin$vectors
print("A * eigenvectors:")
print(res)

# Multiply the eigenvector matrix by the diagonal matrix of eigenvalues.
res2 <- eigen_builtin$vectors %*% diag(eigen_builtin$values)
print("eigenvectors * eigenvalues:")
print(res2)

# Check if these two matrices are equal (they should be equal within numerical precision)
are_equal <- all.equal(res, res2)
print("Are A * eigenvectors and eigenvectors * eigenvalues equal?")
print(are_equal)
```

This example has permitted to understand a variety of concepts related with eigenvalues and eigenvectors, how to calculate it and its importance within redundant variables discovering process.

Being a 2x2 matrix, it will have two eigenvalues. Each eigenvalue has a corresponding eigenvector associated with it. So since the matrix will have two eigenvalues, it will also have two eigenvectors, with 2 values ​​for each vector.

Our table A has next values:

\[X\] \[Y\]

\[1\] 4 2

\[2\] 2 3

The layout of a matrix is ​​defined as the sum of the elements on the main diagonal. In this case, 4 + 3 = 7

We're gonna define temporarily one value called Lambda "λ" and this will be our first eigenvalue.

For instance our **λ** will be = 1

Now we have to make a new matrix called: Matrix ( A - \lambda I )

This new matrix gonna has these values:

\[X\] \[Y\]

\[1\] Matrix A \[X1\] - λ Matrix A \[X2\]

\[2\] Matrix A \[Y1\] Matrix A \[Y2\] - λ

Now calculate the determinant of the matrix ( A - \lambda \* I )

Our matrix ( A - \lambda \* I )

\[X\] \[Y\]

\[1\] 3 2

\[2\] 1 2

At this point the determinant of the matrix ( A - \lambda \* I ) has a value of -2.

determinant formula of the matrix ( A - \lambda \* I ) is:

\[matrix ( A - \lambda \* I )\]\[X1\] - λ \*

(\[matrix ( A - \lambda \* I )\]\[Y2\] - λ) -

\[matrix ( A - \lambda \* I )\]\[Y1\] \* \[matrix ( A - \lambda \* I )\]\[X2\]

In other words: (3-1)\*(2-1)-(2\*2) = -2

But we have an issue! We need to obtain a Lambda value such that this determinant is 0.

Use solver to find the lambda value that makes the determinant 0.

**λ** **= 0.719223608649294**

This λ value is our first eigenvalue.

Before to discover its eigenvector we're gonna know how to get the correct matrix: (A - \lambda\*I), applying **λ** **= 0.719223608649294.**

(I): This is the identity matrix of the same size as (A matrix).

Our matrix ( I )

\[X\] \[Y\]

\[1\] 1 0

\[2\] 0 1

The identity matrix (I) is a square matrix in which all elements on the main diagonal are 1 and all other elements are 0. For a (2 \times 2) matrix, it is: \[ I = \] This matrix is ​​important because, when multiplied by any number, the result is simply scaling the identity matrix, keeping the same shape.

When we multiply the identity matrix (I) by a scalar (in this case, the eigenvalue (\lambda)), each element of the identity matrix is ​​multiplied by (\lambda). This results in: \[ \lambda I = \lambda \cdot \]

Our matrix (lambda\*I)

\[X\] \[Y\]

\[1\] 0.719223608649294 0

\[2\] 0 0.719223608649294

What Does It Mean in the Context of (A - \lambda I)?

Matrix Transformation: The term (\lambda I) affects the matrix (A) by reducing the values ​​on the diagonal, where (\lambda) is subtracted from each of the elements on the main diagonal of (A).

Purpose of the Operation: The equation (A - \lambda I) is key to finding the eigenvalues ​​of the matrix (A). This calculation is based on the concept that, to find the eigenvectors associated with the eigenvalues, you need to solve the equation:

\[(A - \\lambda I) \\cdot X = 0\]

By subtracting (\lambda\* I), you adjust the matrix (A) so that when you multiply it by a vector (X) (finding the eigenvector), the result is the zero vector. This indicates that (X) will not be altered, maintaining its direction under the transformation defined by (A).

In short, (\lambda\* I) transforms the matrix (A) such that by solving the equation ((A - \lambda\*I) \cdot X = 0), the eigenvectors associated with the eigenvalues ​​of the matrix can be identified. This is a fundamental step in the calculation of these components in linear algebra.

Next step: Matrix **(A - \\lambda I)**

\[X\] \[Y\] \[P\] \[Q\]

\[1\] 4 2 - 0.719223608649294 0

\[2\] 2 3 0 0.719223608649294

Matrix **(A - \\lambda I)**

(X1-P1) X2 = (4 - 0.719223608649294) 2 = 3.280776391 2

Y1 (Y2-Q2) 2 (3-0.719223608649294) 2 2.280776391

As a consequence of having obtained this new matrix **(A - \\lambda I)** :

\[X\] \[Y\]

\[1\] 3.280776391 2

\[2\] 2 2.280776391

Having done these calculations, we have simultaneously found the determinant of the matrix. In this case, the value of the determinant was 0.

We need to obtain a Lambda value such that this determinant is 0.

Above we said that we need to obtain a Lambda value such that this determinant is 0 and with λ = 0.719223608649294 we have achieved it.

**But what is the determinant?**

The determinant of a matrix is ​​a scalar value that can be calculated from the matrix's elements.

Outside of the statistical context that I have been developing, for the case of a 2X2 square matrix, the determinant is calculated as follows:

Our table A has had next values:

\[X\] \[Y\]

\[1\] 4 2 = a b

\[2\] 2 3 c d

\[\\text{Det}(A) = ad - bc\] =\> (4\*3)-(2\*2) = 12-4 = 8

For higher dimensional matrices ((3 \times 3) or more), the calculation of the determinant becomes more complex and a method such as cofactor expansion or Sarrus rule for (3 \times 3) matrices is used.

The determinant provides important information about some properties of the matrix, such as:

a\) **Invertibility**: If the determinant of a matrix is ​​zero, the matrix is ​​considered singular and has no inverse. This has important implications in linear algebra and in the solution of systems of linear equations.

If the determinant is nonzero, the matrix is ​​invertible, and its inverse can be calculated, which is critical for various calculations, including those involving eigenvectors.

b\) **Scalability**: The determinant changes predictably under matrix operations.

° If you scale a row of a matrix by a factor (k), the determinant also scales by (k).

° If you swap two rows, the determinant changes sign.

° If you add a multiple of one row to another row, the determinant does not change.

c: **Geometric Interpretation:**

° In the case of (2 x 2) matrices, the determinant can be interpreted as the area of ​​the parallelogram formed by the vectors in the rows or columns of the matrix.

° In (3 x 3) matrices, it can be interpreted as the volume of the parallelepiped formed by the vectors.

Now, the way in which the determinant of the matrix ((A - \lambda\*I)) is calculated follows a similar process to the calculation of the determinant of a (2 \times 2) matrix, but an additional value ((\lambda)) is introduced that complicates the expression.

When we calculate the determinant of ((A - \lambda\*I)), we have to consider how (\lambda\*I) affects the elements of (A).

**1. Estructura de (A - \\lambda\*I)**, **known as the characteristic matrix or eigenvalue matrix**

The identity matrix (I) has (1) on the diagonal and (0) in the other positions, so:The identity matrix (I) has (1) on the diagonal and (0) in the other positions, so:

\[X\] \[Y\]

\[1\] 0.719223608649294 0

\[2\] 0 0.719223608649294

Then the matrix ((A - \lambda I)) becomes:

\[X\] \[Y\]

\[1\] 3.28077639 2 = a b

\[2\] 2 2.28077639 c d

**2. Cálculo del Determinante**:

Now, to calculate the determinant of this new matrix, we apply the same determinant rule (2 \times 2):

(\\text{Det}(A - \\lambda I) = (a - \\lambda)\*(d - \\lambda) - (b \\cdot c))

Expanding this, you get a polynomial in terms of (\lambda):

(\\text{Det}(A - \\lambda I) = ad - a\\lambda - d\\lambda + \\lambda\^2 - bc)

Which simplifies to:

(\\text{Det}(A - \\lambda I) = \\lambda\^2 - (a + d)\\lambda + (ad - bc))

Then:

\[X\] \[Y\]

\[1\] 3.280776391 2

\[2\] 2 2.280776391

Det(A - \\lambda I) = ((0.719223608649294)\^2) - ((3.280776391 + 2.280776391) \* (0.719223608649294)) + ((3.280776391 \* 2.280776391) - (2\*2))

Det(A - \\lambda I) = -0.0000001241, **that's it 0**

**Summary of the Difference between determinant of (A) and (A-(** **λ\*I))**

**Characteristic Polynomial:** The determinant of ((A - \lambda I)) is a polynomial in (\lambda), which allows us to *find the eigenvalues* ​​by solving the characteristic equation by equating that polynomial to zero. **Incorporating (**\lambda\*\*):\*\* The presence of (\lambda) in the matrix affects how we calculate the determinant and produces a second-degree polynomial in the case of (2 \times 2) matrices.

**Relationship between Determinants and Eigenvectors**

° The characteristic polynomial, obtained by calculating the determinant of ( (A - \lambda I) ), is used to find the eigenvalues ​​of (A).

The characteristic polynomial of a square matrix is ​​a polynomial obtained by calculating the determinant of the matrix ( (A - \lambda I) ), where:

\(A\) is the original matrix.

(\lambda) is a scalar (the eigenvalue we are looking for.

\(I\) is the identity matrix of the same dimension as (A).

The determinant is used in the construction of the characteristic polynomial. To find the eigenvalues ​​of a matrix, you need to solve the characteristic polynomial equation:

{Det}(A - \lambda I) = 0

The result of this determinant is a polynomial in terms of (\lambda). The roots of this polynomial (the values ​​of (\lambda)) are the eigenvalues ​​of the matrix (A).

° By calculating the determinant, you can understand the structure of the matrix and determine properties that are essential for analyzing its eigenvalues ​​and eigenvectors.

**Integrating concepts**

By defining the characteristic polynomial equation \[ \text{Det}(A - \lambda I) = 0 \] in terms of a lambda, I am looking for the result to be 0. If I find a lambda, such that the result of the characteristic polynomial equation is 0, then that value 0, is the determinant of the characteristic matrix (A - \lambda\*I) and the value of lambda, becomes the first eigenvalue.

Now we're gonna **find its iegenvector** through this next formula:

***\[(A - \\lambda\*I) \\cdot\*X = 0\]***

Breakdown of the Equation

(A): This is the original matrix from which you are calculating the eigenvalues ​​and eigenvectors. (\lambda): This is the eigenvalue.

(I): This is the identity matrix of the same size as (A).

(X): This is the eigenvector we are looking for.

**Steps to Find the Eigenvector**

Matrix A = (4 22 3)

First eigenvalue: \[**λ =** 0.719223608649294\]

**Step 1:** Build the Matrix (A - **λ**I)

\[X\] \[Y\]

\[1\] 3.280776391350706 2

\[2\] 2 2.280776391350706

**Step 2:** The equation you want to solve is:

\[(A - **λ**I) \* X = 0\]

This translates to a system of linear equations, where (X) is the eigenvector we are looking for. Let us denote the eigenvector as:

\[X = (x1 x2)\]

**Step 3:** Establish the System of Equations

This leads you to the two equations from the matrix (A - \lambda I):

For the first row:

\[3.280776391350706 \* x1 + 2 \* x2 = 0\] =\> (1)

For the second row:

\[2 \* x1 + 2.280776391350706 \* x2 = 0\] =\> (2)

**Step 4:** Solve the System of Equations

From the first equation (1):

$$
[
2 \cdot x_2 = -3.280776391350706 \cdot x_1 \quad \Rightarrow \quad x_2 = -\frac{3.280776391350706}{2} \cdot x_1
]
$$

Substituting in the second equation:

$$
[
2 \cdot x_1 + 2.280776391350706 \cdot \left(-\frac{3.280776391350706}{2} \cdot x_1\right) = 0
]
$$

Simplify:

$$
[2 \cdot x_1 - 3.280776391350706 \cdot x_1 = 0]
$$$$
[x_1 \cdot (2 - 3.280776391350706) = 0]
$$

**Step 5:** Choose a Value of (x_1)

We can select any value for (x_1) (usually (1)) and then calculate (x_2).

Here, we may have one or both components of the result other than 0. To consider the values ​​x_1 and x_2 to be the eigenvector of the eigenvalue lambda_1, they must be 0. Now, an important clarification needs to be made here.

The values ​​x_1 and x_2 that satisfy an eigenvector must necessarily generate components with the value 0.00?

Isn't it sufficient that both calculated components are very close to 0.00?

In the context of linear algebra and eigenvector calculus, when the equation is verified:

\[(A - **λ**I) \* X = 0\]

The result must be exactly the zero vector. However, in practice, approximation criteria are used due to the limits of numerical precision in computations, especially when working with software and numerical calculations.

**Details of the Approximation Criterion**

1.  *Exact Zero Vector:*

Theoretically, an eigenvector (X) must satisfy the requirement that the product ((A - \lambda I) \cdot X) equals exactly (0 0).

This tells us that the vector (X) is a solution to the system of equations derived from the relation.

2.  *Approximations in Calculations:*

In practice, when performing calculations, especially in software (such as R, Python, MATLAB, etc.), it is common for the result to not be exactly zero due to:

**Numerical errors:** Inaccurate calculations due to rounding or precision of floating-point numbers. **Statistical noise:** In real-world data analysis, there may be variations in the data that introduce slight errors.

3.  *Criterio de Tolerancia:*

If the result is close to zero (for example, less than a specific threshold like (0.001) or (0.0001)), you can consider that the vector meets the condition to be an eigenvector.

This approach is used because in computations in R, Python, or MATLAB, numerical representations may not be exact, even though the model behavior may be correct.

**Recapitulating the Steps to Find the Eigenvector**

Trying to find values ​​for x_1 and x_2 that satisfy the requirement that the result of each equation or component be 0 can be very laborious to do by hand. Therefore, to speed up the process, I used a Microsoft Excel tool.

In the ribbon =\> Data, section =\> Forecast, we go to =\> Hypothesis analysis and finally =\> Goal seek...

Using this tool will give us a value x_1 that seeks to approximate the result of component 1 to 0. But since the result of the function of component 1 directly intervenes in the result of component 2 because of exist a dependency relationship, we must also apply the "Excel goal search" process, to find the value of x_2. This process must be iterated several times until Excel indicates that it has reached the highest degree of precision, that is, as close to 0 as possible.

$$
[(A - \lambda I) \cdot \begin{pmatrix}x_1 \\x_2\end{pmatrix} = 0]
$$

$$
[A - \lambda I = \begin{pmatrix}4 - \lambda & 2 \\2 & 3 - \lambda\end{pmatrix}]
$$

$$
[\begin{pmatrix}x_1 \\x_2\end{pmatrix} = \begin{pmatrix}-0.001033832 \\0.001314788\end{pmatrix}]
$$

For the first row:

\[3.280776391350706 \* -0.001033832 + 2 \* 0.001314788 = 0\] =\> (**-0.000762196**)

For the second row:

\[2 \* -0.001033832 + 2.280776391350706 \* 0.001314788 = 0\] =\> (**0.000931073**)

In my case, I managed to find x_1: -0.00103383179577251 and x_2: 0.00131478761475097. In this way I obtained component_1: -0.000762196 and component_2: 0.000931073.

Now I check if the components are within the tolerance range used for this type of calculations: \[-0.001, 0.001\]

*First component:* (-0.000762708) is within tolerance (\[-0.001, 0.001\]).

*Second component:* (0.000932849) is also within tolerance (\[-0.001, 0.001\]).

Since both components are within the specified tolerance range, we can consider that the vector

(X = -0.001033832 0.001314788) is effectively an eigenvector associated with the eigenvalue (\lambda\_1 = 0.719223608649294).

**Search for the second eigenvalue**

Mencione que el segundo valor propio se obtiene restando la suma de la trayectoria de la matriz A - Lambda_1.

La trayectoria de una matriz se define como la suma de los elementos de la diagonal principal. En este caso, 4 + 3 = 7.

So:

7 - 0.719223608649294 = 6.280776391 =\> **Lambda_2**

Now the task remains to apply all the same procedures seen to find the eigenvector for lambda_2.

**Additional resources used in the development of this research:**

<https://latexeditor.lagrida.com/>

Microsoft Excel

**Conclusions on the calculation of eigenvalues ​​and eigenvectors in a 2X2 matrix**

1.  Eigenvalues ​​and Redundant Variables

*Characteristic Polynomial:* By calculating the characteristic polynomial of a matrix, you are examining the algebraic structure of the relationships between the variables. The eigenvalues ​​you obtain are indicative of how the variables interact.

An eigenvalue with a magnitude very close to zero (or actually zero) generally indicates a linear combination of variables that contributes no new information to the data set. This suggests that the variable or combination of variables is redundant.

2.  Linear Dependence

*Null Space:* Eigenvectors corresponding to eigenvalues ​​that are zero indicate that there are linear dependencies between variables. If you can express a variable as a combination of other variables, this means you have redundancy in your dataset.

*Collinearity:* The fact that a variable can be represented by other variables indicates that it has a collinear relationship, which can complicate regression models and make the coefficients unstable.

3.  Principal Component Analysis (PCA)

In the context of data analysis, eigenvalues ​​are used in PCA to identify directions of maximum variance in the data.

Directions (principal components) with eigenvalues ​​close to zero can be viewed as feature components that do not add significant value to the data set, suggesting that you can drop one or more of the original variables to maintain a simpler and more effective model.

4.  Dimensionality Reduction

*Model Simplification:* By identifying and eliminating redundant variables (through eigenvalue analysis), you can reduce the dimensionality of the model without losing critical information. This can facilitate interpretation and improve the accuracy of predictive models.

**Conclusions applied to matrix A**

The first eigenvalue or lambda_1 is: 0.719223608649294

The second eigenvalue or lambda_2 is: 6.280776391

Lambda_1 indicates that there is a linear combination of the variables that does not contribute new information to the data set.

**Eigenvalue (**\lambda\*\*\_1):\*\*

The eigenvalue (\lambda\_1 = 0.719223608649294) suggests that it is a significant eigenvalue; however, compared to (\lambda\_2), which is larger, it may indicate that the direction associated with (\lambda\_1) is less important or that there is a linear combination of the variables that does not contribute significantly to the total variance of the data set.

**Meaning of (**\lambda\*\*\_1):\*\*

When you find a small eigenvalue (close to zero), it typically indicates collinearity or linear dependence between the variables. In other words, at least one variable may not be contributing unique information to the model, as it can be represented or explained by a combination of the other variables.

**Interpretation of Eigenvalues**

*Linear Dependencies and Contributions:*

A small eigenvalue (such as (\lambda\_1)) generally indicates that the direction associated with this value does not explain a large portion of the total variance in the data set.

A small eigenvalue (closer to zero) suggests that the variables in the matrix are, in some sense, "trapped" in a linear relationship, and one or more variables do not contribute significant additional information.

*Collinearity:*

In the case of your matrix (A), if we observe that (\lambda\_1 = 0.719223608649294) is relatively small compared to (\lambda\_2), this indicates that the direction associated with the smaller eigenvalue (in vector space) does not add value to the same extent as the direction associated with the larger eigenvalue.

This may indicate that there may be some linear dependence between the variables that form that direction; it is indicative of collinearity.

**What is the variable in question? / Which variable or variables are redundant?**

In practical terms, if for example, one of the eigenvectors associated with (\lambda\_1) suggests that a component (such as a combination of variables (X) and (Y) in your matrix) contributes very little to the variance, then that might be the component that is redundant or not significant.

**Evaluate variables in this context**

***Correlation Analysis:*** You can calculate the correlation matrix to see how variables relate to each other. It will allow you to see which variables are highly correlated and could be offsetting the impact between them.

***PCA:*** By performing principal components analysis (PCA), you can identify which components account for the majority of the variance and which ones can be discarded. It will help identify how many dimensions are necessary to explain the variability of the data and if there are components that explain little variance, suggesting redundancies.

The eigenvalue (\lambda\_1 = 0.719223608649294) is the key in this case to identifying the direction that might be generating the least impact on the total variance; the eigenvector associated with this value may correspond to a combination of your variables that doesn't contribute significantly. This suggests that, when analyzing the variables in your data set, you might consider further examining those related to this eigenvalue.

Finally the relationships between eigenvalues ​​and eigenvectors, and the way they are analyzed, are key to understanding redundancy in data. Performing these calculations not only helps identify which variables are contributing significantly to the model, but also helps reduce the risk of multicollinearity and improve the accuracy of any analysis performed.

------------------------------------------------------------------------

### Redundant Variables Detection

```{r}
#| echo: true
#| message: false
#| warning: false

# Covariance matrix 
cm_delitos_data <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  select(-sum_24TR, -sum_24SE, -sum_24SS) %>%
  cov()

# Compute eigenvalues and eigenvectors
eigen_results <- cm_delitos_data %>% eigen()

# Extract eigenvalues and eigenvectors
eigenvalues <- eigen_results$values
eigenvectors <- eigen_results$vectors

# Display eigenvalues and eigenvectors
print(eigenvalues)
head(eigenvectors)

# The Smallest Eigenvalues
sort(eigenvalues, decreasing = FALSE)

# The smallest eigenvalue is approximately zero
smallest_eigenvalue <- min(eigenvalues)
print(smallest_eigenvalue)

# Corresponding eigenvector
smallest_eigenvector <- eigenvectors[, which.min(eigenvalues)]
print(smallest_eigenvector)

# Normalize the eigenvector by dividing by the largest absolute value
normalized_eigenvector <- smallest_eigenvector / max(abs(smallest_eigenvector))
print(normalized_eigenvector)

# Sorted normalize the eigenvector
sort(abs(normalized_eigenvector), decreasing = T)

# Get numeric variable names (order matches eigenvector indices)
variable_names <- colnames(cm_delitos_data)

# Sort normalized eigenvector by absolute contribution (descending order)
sorted_contributions <- sort(abs(normalized_eigenvector), decreasing = TRUE)

# Get the indices of the top contributions
top_indices <- order(abs(normalized_eigenvector), decreasing = TRUE)

# Get the names of the top variables
top_variable_names <- variable_names[top_indices]

# Print the top variable names
print(top_variable_names)
```

**Valle del Cauca**

**Prompt** :

What is: Redundant Variables?

They refer to those variables that do not provide substantial new information because their content is already represented by other variables in the data set.

Collinearity: It occurs when two or more variables are highly correlated. For example, if you have both height variables in centimeters and meters, one is simply a conversion of the other.

Duplication: Often, a variable can be a direct copy or a simple transformation of another variable in the data set. This increases dimensionality without adding analytical value.

For instance: If you are evaluating the weight of people in a data set and you have both kilograms and pounds, one variable is redundant.

### Regression Analysis

```{r}
# Fit a regression model to confirm the relationship
model <- lm(sum_24HP ~ sum_24HA + sum_24EX + sum_24HM + sum_24LP + sum_24HR + sum_24VI + sum_24HC + sum_24HOM + sum_24DS, 
            data = data.frame(delitos_data))

## AQUÍ TIENE QUE SER FRENTE A TOOOOODAS LAS VARIABLES, NO SOLO LAS DE LOS EIGEN VALUES MÁS BAJOS 

# model <- lm(sum_24SE ~ sum_24DS + sum_24EX + sum_24SS + sum_24LP + 
#               sum_24HOM + sum_24HR + sum_24VI + sum_24HM + 
#               sum_24HA + sum_24HP + sum_24HC, 
#             data = data.frame(delitos_data))
# 
# model <- lm(sum_24SS ~ sum_24EX + sum_24HC + sum_24HR + 
#                   sum_24HA + sum_24DS + sum_24HM + 
#                   sum_24HOM + sum_24HP + sum_24VI + sum_24LP,
#                 data = data.frame(delitos_data))
# 
# model <- lm(sum_24HOM ~ sum_24HA + sum_24EX + sum_24HM +
#               sum_24LP + sum_24HR + sum_24VI +
#               sum_24HC + sum_24HP + sum_24DS,
#             data = data.frame(delitos_data))

summary(model)

# Variance Inflation Factors
vif(model)
```

Conclusion: As we have seen, the p-values ​​\[Pr(\>\|t\|)\] from the linear regression show a result of "2.00E-16", that is, 0.15 zeros and 2.

This means that with the exception of the independent variable "sum_24HOM", all p-values ​​(0.0000000000000002) are less than the significance level defined for alpha, commonly: (\\alpha = 0.05) or (\\alpha = 0.01). Therefore, the Null Hypothesis (H_0) is rejected, which suggests that there is sufficient evidence to assume that (X) has an effect on (Y).

By rejecting the null hypothesis, you accept the alternative hypothesis ((H_a)), which suggests that the coefficient is not zero and that the independent variable does have an effect on (Y). Or what would be the same, "sum_24HOM" is the only one that does not have a statistically significant relationship or effect on (Y).

In my coefficient table or model summary, I have, on the one hand, the intercept (denoted as (B_0)) with an estimated value of (0.096096). This intercept represents the value of the dependent variable ((sum\_{24HP})) when all of the independent variables ((sum\_{24HA}), (sum\_{24EX}), (sum\_{24HM}), (sum\_{24LP}), (sum\_{24HR}), (sum\_{24VI}), (sum\_{24HC}), (sum\_{24HOM}), and (sum\_{24DS})) are equal to zero.

On the other hand, for the remaining independent variables, each estimated coefficient ((B_i)) indicates the effect of that variable on (sum\_{24HP}), that is, the expected amount of change in the dependent variable for each unit of change in the respective independent variable, holding all other variables constant.

The estimated coefficient for each independent variable represents the slope associated with that specific variable in the regression relationship. Therefore, for each unit change in the independent variable (B_i), the value of (sum\_{24HP}) adjusts by that coefficient, but the intercept ((B_0)) remains constant and does not change as a result of changes in (B_i).

------------------------------------------------------------------------

## Global Variability Metric

```{r}
#| echo: true
#| message: false
#| warning: false

cov_matrix <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  #select(-sum_24TR, -sum_24SE, -sum_24SS) %>%
  cov() 

# Effective Variance
det(cov_matrix)^(1/ncol(cov_matrix))

# Log-Transformed Effective Variance
det(log(cov_matrix + 1))^(1/ncol(cov_matrix))

# Effective Standard Deviation
det(cov_matrix)^(1/(ncol(cov_matrix) * 2))

# Log-Transformed Effective Standard Deviation
det(log(cov_matrix + 1))^(1/(ncol(cov_matrix) * 2))
```

------------------------------------------------------------------------

**Valle del Cauca:**

```{r}
Resúmen Estadístico delitos_data$sum_24HP
Medida	Valor
Media	0.26164840
Mediana	0.00000000
Moda	0.00000000
Varianza	1.32068850
Desviación Estándar	1.14921210
1er Cuartil	0.00000000
2do Cuartil (Mediana)	0.00000000
3er Cuartil	0.00000000
```

Media: 0.26164840

```{r}
Covariance Matrix

sum_24HOM	sum_24LP	sum_24VI	sum_24DS	sum_24HP	sum_24HR	sum_24HC	sum_24HA	sum_24HM	sum_24SS	sum_24SE	sum_24EX	sum_24TR
sum_24HOM	0.02	0.01	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0	0	0.00	0
sum_24LP	0.01	0.23	0.09	0.02	0.13	0.01	0.01	0.00	0.02	0	0	0.01	0
sum_24VI	0.00	0.09	0.40	0.05	0.18	0.01	0.02	0.01	0.02	0	0	0.02	0
sum_24DS	0.00	0.02	0.05	0.08	0.05	0.00	0.01	0.00	0.00	0	0	0.01	0
sum_24HP	0.00	0.13	0.18	0.05	1.32	0.02	0.18	0.03	0.06	0	0	0.02	0
sum_24HR	0.00	0.01	0.01	0.00	0.02	0.03	0.00	0.00	0.00	0	0	0.00	0
sum_24HC	0.00	0.01	0.02	0.01	0.18	0.00	0.11	0.01	0.01	0	0	0.01	0
sum_24HA	0.00	0.00	0.01	0.00	0.03	0.00	0.01	0.02	0.00	0	0	0.00	0
sum_24HM	0.00	0.02	0.02	0.00	0.06	0.00	0.01	0.00	0.05	0	0	0.00	0
sum_24SS	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0	0	0.00	0
sum_24SE	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0	0	0.00	0
sum_24EX	0.00	0.01	0.02	0.01	0.02	0.00	0.01	0.00	0.00	0	0	0.02	0
sum_24TR	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0.00	0	0	0.00	0
```

```{r}
> cov_matrix <- delitos_data %>%
+   st_drop_geometry() %>%
+   select(contains('24')) %>%
+   #select(-sum_24TR, -sum_24SE, -sum_24SS) %>%
+   cov()
>   round(2) %>%
+   knitr::kable(digits = 2, caption = "Covariance Matrix")
> 
> # Effective Variance
> det(cov_matrix)^(1/ncol(cov_matrix))
[1] 0.01707155
> 
> # Log-Transformed Effective Variance
> det(log(cov_matrix + 1))^(1/ncol(cov_matrix))
[1] 0.01558223
> 
> # Effective Standard Deviation
> det(cov_matrix)^(1/(ncol(cov_matrix) * 2))
[1] 0.1306581
> 
> # Log-Transformed Effective Standard Deviation
> det(log(cov_matrix + 1))^(1/(ncol(cov_matrix) * 2))
[1] 0.1248288
```

Effective variance and effective standard deviation are measures that reflect the variability in a data set. In this case, they are derived from the determinant of the covariance matrix, which measures how variables vary relative to one another.

Covariance Matrix The covariance matrix is ​​fundamental in statistics because it captures the covariance between multiple variables. It shows how these variables change together.

1.  **Effective Variance:** 0.01707155

    det(cov_matrix): Calculates the determinant of the covariance matrix.

    ```         
    > xxx <- det(cov_matrix) > xxx [1] 1.046037e-23
    ```

    Expression: we raise the determinant to:

    $$
    (\Large\frac{1}{ncol(cov_matrix)} )
    $$

    to calculate the effective variance in the data set, calculated from the determinant of the covariance matrix. This measure gives an indicator of the joint variability of the variables in the dataset. It indicates how much joint variability the variables transmit in the set when considering their interactions and individual variances.

2.  **Log-Transformed Effective Variance:** 0.01558223

    Similar to the previous calculation, but working with the log-transformed covariance matrix. This is done by first adding 1 to avoid problems with logarithms of zero. This value indicates the effective variance of the variables after applying the logarithmic transformation.

    Using log-transformed data can help reduce the influence of outliers and improve the normality of the data.

3.  **Effective Standard Deviation:** 0.1306581

    This calculation is used to determine the effective standard deviation and is based on the determinant of the covariance matrix. Raising the determinant to

$$
(\Large\frac{1}{(ncol(cov_matrix) * 2)})
$$

gives the square root of the effective variance. Provides a measure of the dispersion in the variables in the data set.

4.  **Log-Transformed Effective Standard Deviation:** 0.1248288

    Here, the effective standard deviation is calculated using the log-transformed covariance matrix, using the same approach as before.

    This value is the effective standard deviation calculated from the covariance matrix after applying the logarithmic transformation. As with variance, this transformation can help provide a better representation of dispersion, especially in data sets that may not follow a normal distribution.

------------------------------------------------------------------------

## Linear Dependency and Precision Matrix

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute precision matrix
S_inv <- solve(cov_matrix)

# Display precision matrix (should match example values)
cat("Precision Matrix (S⁻¹):\n")
print(S_inv, digits = 2)

# Extract correct row components of the selected crime
dependent_variable_index <- 5

first_row <- S_inv[dependent_variable_index, ]
print(first_row, digits = 2)

diag_element <- S_inv[dependent_variable_index, dependent_variable_index]
print(diag_element, digits = 2)

# Compute regression coefficients
beta_coefficients <- -first_row[-dependent_variable_index] / diag_element
print(beta_coefficients, digits = 2)

# Compute residual variance
residual_variance <- 1 / diag_element
residual_sd <- sqrt(residual_variance)  # Residual standard error

# Print residual standard error
print(residual_sd, digits = 2)

# Compute R^2
r_squared <- 1 - (residual_variance / cov_matrix[dependent_variable_index, dependent_variable_index])
print(r_squared, digits = 2)

# Verify with lm() regression
delitos <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  select(-sum_24TR, -sum_24SE, -sum_24SS)

# Fit model
model <- lm(sum_24HP ~ ., data = data.frame(delitos))
summary(model)
```

**Valle del Cauca:**

Precision Matrix: is the inverse of the covariance matrix (S). The precision matrix provides information about the conditional dependencies between variables. If one entry is zero, the variables are conditionally independent.

```{r}
Precision Matrix (S⁻¹):
          sum_24HOM sum_24LP sum_24VI sum_24DS sum_24HP sum_24HR sum_24HC sum_24HA sum_24HM sum_24EX
sum_24HOM   42.8134   -1.349   -0.099     0.11    0.039   -0.253    -0.12   0.0015    -0.48   -0.251
sum_24LP    -1.3492    4.905   -0.806    -0.30   -0.311   -0.479     0.14   0.0337    -0.65   -0.930
sum_24VI    -0.0985   -0.806    3.096    -1.53   -0.236   -0.481     0.18  -0.3041    -0.76   -1.181
sum_24DS     0.1088   -0.298   -1.530    14.38   -0.232    0.107    -0.16   0.2510     0.24   -2.714
sum_24HP     0.0385   -0.311   -0.236    -0.23    1.107   -0.335    -1.58  -0.8382    -0.72   -0.335
sum_24HR    -0.2532   -0.479   -0.481     0.11   -0.335   36.413     0.39  -1.7344    -0.59   -0.034
sum_24HC    -0.1179    0.144    0.181    -0.16   -1.584    0.392    11.90  -1.6704    -0.29   -1.453
sum_24HA     0.0015    0.034   -0.304     0.25   -0.838   -1.734    -1.67  50.7340    -1.36   -0.269
sum_24HM    -0.4845   -0.654   -0.755     0.24   -0.718   -0.590    -0.29  -1.3580    20.18   -0.612
sum_24EX    -0.2508   -0.930   -1.181    -2.71   -0.335   -0.034    -1.45  -0.2690    -0.61   52.453
```

**Diagonal Values:**

The entries on the diagonal (e.g., (42.8134) for sum_24HOM, (4.905) for sum_24LP, etc.) represent the conditional variance of each variable.

These values ​​indicate how much each variable varies relative to its relationship with all other variables in the model. A higher value implies that the variable has greater conditional variability.

**Off-Diagonal Values**

Off-diagonal entries represent the conditional covariances between variables. For example:

The value (-1.3492) in the sum_24LP row and the sum_24HOM column indicates a negative relationship between these two variables. Overall, this suggests that an increase in sum_24HOM is associated with a decrease in sum_24LP, after controlling for the effect of the other variables.

A value closer to 0 indicates that there is no strong relationship between these variables in conditional terms.

**Prompt** :

What can I conclude from having 42.8134 in the first diagonal position for the variable sum_24HOM, which is the highest value in this precision matrix?

*High Conditional Variance:* a value of 42.8134 indicates that the conditional variance of sum_24HOM is high when all other variables in the model are controlled. This means that sum_24HOM shows a large amount of variability in its relationship with the dependent variable sum\_{24HP} in the context of the other variables.

*Contribución Significativa:* since this is the highest value on the diagonal, it suggests that sum_24HOM makes a significant contribution to the total variance of the data set. In other words, the variability of sum_24HOM is an important factor in the model and has a noticeable impact on how changes in sum\_{24HP} are explained.

Low Collinearity: the high conditional variance may also suggest that sum_24HOM is not strongly correlated with the other variables, meaning it contributes unique information to the model. It is not necessarily redundant, as its variability is not locked into the other variables.

Importance for the Model: this indicates that sum_24HOM is an important variable to consider when building predictive models. The high conditional variance suggests that changes in sum_24HOM will likely significantly influence the dependent variable.

```{r}
# Extract correct row components of the selected crime
> dependent_variable_index <- 5
> 
> first_row <- S_inv[dependent_variable_index, ]
> print(first_row, digits = 2)
sum_24HOM  sum_24LP  sum_24VI  sum_24DS  sum_24HP  sum_24HR  sum_24HC  sum_24HA  sum_24HM  sum_24EX 
    0.039    -0.311    -0.236    -0.232     1.107    -0.335    -1.584    -0.838    -0.718    -0.335 
```

Extract the components of the row corresponding to the dependent variable (sum\_{24HP}) in the precision matrix. This will allow you to see how (sum\_{24HP}) relates to the other variables.

```{r}
#Calculation of the Regression Coefficient
diag_element <- S_inv[dependent_variable_index, dependent_variable_index]
> print(diag_element, digits = 2)
[1] 1.1
```

Calculation of the Regression Coefficient: This extracts the diagonal element of the precision matrix for (sum\_{24HP}) which will be used to calculate the regression coefficients.

```{r}
# Compute regression coefficients
> beta_coefficients <- -first_row[-dependent_variable_index] / diag_element
> print(beta_coefficients, digits = 2)
sum_24HOM  sum_24LP  sum_24VI  sum_24DS  sum_24HR  sum_24HC  sum_24HA  sum_24HM  sum_24EX 
   -0.035     0.281     0.213     0.210     0.303     1.432     0.757     0.649     0.303 
```

This calculation line converts the extracted row from the precision matrix into regression coefficients. For each value in first_row, you divide it by the diagonal value of the calculated element, resulting in the regression model coefficients.

```{r}
> # Compute residual variance
> # Compute residual variance
> residual_variance <- 1 / diag_element
> residual_sd <- sqrt(residual_variance)  # Residual standard error
> 
> # Print residual standard error
> print(residual_sd, digits = 2)
[1] 0.95
```

Residual Variance: calculate the variance of the residuals from the diagonal element of the precision matrix. The square root of this variance (residual_sd) will give you the standard error of the residuals in the model.

```{r}
> # Compute R^2
> r_squared <- 1 - (residual_variance / cov_matrix[dependent_variable_index, dependent_variable_index])
> print(r_squared, digits = 2)
[1] 0.32
```

Compute R\^2: It is calculated as the proportion of variance explained relative to the total variance. This indicates how well the model fits the data.

```{r}
> # Verify with lm() regression
> delitos <- delitos_data %>%
+   st_drop_geometry() %>%
+   select(contains('24')) %>%
+   select(-sum_24TR, -sum_24SE, -sum_24SS)
> 
> # Fit model
> model <- lm(sum_24HP ~ ., data = data.frame(delitos))
> summary(model)

Call:
lm(formula = sum_24HP ~ ., data = data.frame(delitos))

Residuals:
    Min      1Q  Median      3Q     Max 
-19.311  -0.096  -0.096  -0.096  74.195 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.096096   0.004753  20.219   <2e-16 ***
sum_24HOM   -0.034799   0.029218  -1.191    0.234    
sum_24LP     0.280697   0.009801  28.639   <2e-16 ***
sum_24VI     0.212884   0.007793  27.317   <2e-16 ***
sum_24DS     0.209750   0.016907  12.406   <2e-16 ***
sum_24HR     0.302510   0.026909  11.242   <2e-16 ***
sum_24HC     1.431535   0.013855 103.320   <2e-16 ***
sum_24HA     0.757449   0.031607  23.965   <2e-16 ***
sum_24HM     0.648806   0.019827  32.723   <2e-16 ***
sum_24EX     0.302932   0.032309   9.376   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9507 on 45318 degrees of freedom
Multiple R-squared:  0.3158,	Adjusted R-squared:  0.3156 
F-statistic:  2324 on 9 and 45318 DF,  p-value: < 2.2e-16
```

Verification with lm() of Regression: finally, these results are compared with those obtained by fitting a regression model with lm(). This gives you a summary that includes the coefficients, standard errors and (R\^2), allowing you to verify the consistency of your results.

------------------------------------------------------------------------

# Key Players and Topics

# Non-Parametric Correlation {.unnumbered}

Correlation measures the strength and direction of association between two variables. While Pearson's correlation requires a linear relationship and normally distributed data, \emph{Spearman's rank correlation} and \emph{Kendall's tau} are \emph{non-parametric} measures, making them ideal for analyzing data that may not be linear or normally distributed.

## Spearman's Rank Correlation

Spearman's correlation coefficient $\rho$ is based on \emph{ranked} data. For two variables $X$ and $Y$, we replace each observation by its rank.

```{r}
#| include: false
#---source('setup.R')

# Load the dataset
#---delitos_data <- st_read("data/spatial/crime_spatial_course.gpkg")
#---delitos_data <- delitos_data[delitos_data$dpto_ccdgo == '11', ]
```

```{r}
#| echo: true
#| message: false
#| warning: false

delitos_data <- delitos_data %>% 
  select(-sum_24TR, -sum_24SE, -sum_24SS)

delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24')) %>%
  cor(., method = "spearman", use = "complete.obs") %>%
  round(., 3) %>% 
  print(.) %>%
  corrplot(., method = "color", title = "Spearman Correlation", mar=c(0,0,1,0))
```

**Valle del Cauca:**

```         
+   corrplot(., method = "color", title = "Spearman Correlation", mar=c(0,0,1,0))
          sum_24HOM sum_24LP sum_24VI sum_24DS sum_24HP sum_24HR sum_24HC sum_24HA sum_24HM sum_24EX
sum_24HOM     1.000    0.092    0.040    0.022    0.045    0.016    0.030    0.010    0.020    0.027
sum_24LP      0.092    1.000    0.138    0.067    0.151    0.060    0.074    0.037    0.097    0.065
sum_24VI      0.040    0.138    1.000    0.091    0.149    0.058    0.069    0.055    0.087    0.062
sum_24DS      0.022    0.067    0.091    1.000    0.086    0.035    0.044    0.032    0.061    0.045
sum_24HP      0.045    0.151    0.149    0.086    1.000    0.098    0.170    0.122    0.156    0.085
sum_24HR      0.016    0.060    0.058    0.035    0.098    1.000    0.036    0.056    0.040    0.021
sum_24HC      0.030    0.074    0.069    0.044    0.170    0.036    1.000    0.049    0.087    0.062
sum_24HA      0.010    0.037    0.055    0.032    0.122    0.056    0.049    1.000    0.059    0.024
sum_24HM      0.020    0.097    0.087    0.061    0.156    0.040    0.087    0.059    1.000    0.042
sum_24EX      0.027    0.065    0.062    0.045    0.085    0.021    0.062    0.024    0.042    1.000
```

Spearman's rank correlation is the nonparametric counterpart of Pearson's correlation. It does not use the original data, but rather the data ranges.

The correlation matrix shows how each pair of variables is related and how to interpret the coefficients between -1 and 1, where:

\(1\) Indicates a perfect positive correlation,

(-1) Indicates a perfect negative correlation,

\(0\) Indicates no correlation.

Each correlation index gives us three specific pieces of information.

Direction: If the Rho is positive, the correlation is positive.

Strength: The closer the correlation is to 1, the greater the strength, i.e., the stronger the association between the variables.

Significance: If we take another sample from the same population, we will likely find the same association.

So, taking on account the table we can conclude:

sum_24HP and sum_24HC: They have a correlation of (0.170), which is the strongest correlation observed in this matrix. This suggests a moderate positive relationship between these two variables; as one increases, the other tends to increase.

sum_24HP and sum_24LP: A correlation of (0.151), also positive, indicating a moderate positive relationship between these two variables.

All other correlations are quite low, reinforcing the idea that none of the variables are strongly related to the others. This indicates that there is no significant redundancy in the variables, since no strong correlations are observed that suggest linear dependence.

------------------------------------------------------------------------

# Spatial Neighborhood Matrices {.unnumbered}

## Neighbors Based on Contiguity

```{r}
# Create a spatial neighbors list using Queen contiguity
# (i.e., polygons are considered neighbors if they share any point: edge or vertex)
nb <- spdep::poly2nb(delitos_data, queen = TRUE)
head(nb)

# Replace invalid neighbor entries (i.e., [1] 0) with empty integer vectors
# This ensures compatibility with functions that expect valid neighbor lists only
nb_0 <- lapply(nb, function(x) if(length(x)==1 && x==0) integer(0) else x)

# Polygons with neighbors
table(sapply(nb_0, length))

# Neighbors of Order k Based on Contiguity
# Neighbors of second order
nblags <- spdep::nblag(neighbours = nb, maxlag = 2)

# Combine neighbors of all orders up to the specified lag (in this case, up to order 2)
# This creates a cumulative neighbor list including first- and second-order neighbors
nblagsc <- spdep::nblag_cumul(nblags)
table(sapply(nblagsc, length))
```

**Valle del Cauca:**

```{r}
> # Create a spatial neighbors list using Queen contiguity
> # (i.e., polygons are considered neighbors if they share any point: edge or vertex)
> nb <- spdep::poly2nb(delitos_data, queen = TRUE)
> head(nb)
[[1]]
[1] 0

[[2]]
[1] 0

[[3]]
[1] 0

[[4]]
[1] 0

[[5]]
[1] 0

[[6]]
[1] 35204

> 
> # Replace invalid neighbor entries (i.e., [1] 0) with empty integer vectors
> # This ensures compatibility with functions that expect valid neighbor lists only
> nb_0 <- lapply(nb, function(x) if(length(x)==1 && x==0) integer(0) else x)
> 
> # Polygons with neighbors
> table(sapply(nb_0, length))

    0     1     2     3     4     5     6     7     8     9    10    11    13    14    23    29 
40279  3630   886   275   116    50    36    21    12     9     4     2     2     3     2     1 
> 
> # Neighbors of Order k Based on Contiguity
> # Neighbors of second order
> nblags <- spdep::nblag(neighbours = nb, maxlag = 2)
> 
> # Combine neighbors of all orders up to the specified lag (in this case, up to order 2)
> # This creates a cumulative neighbor list including first- and second-order neighbors
> nblagsc <- spdep::nblag_cumul(nblags)
> table(sapply(nblagsc, length))

    1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19    20    22    23    24    26    29 
42131   984   595   456   282   228   176   110   100    54    35    13    26    30    10     2     2     2     5     4     2    45     4     2    30
```

After creating the Neighbor List using Queen's Contiguity, any pair of two polygons that share a point, either on the boundary or a vertex, are considered neighbors. The result shows that the first five polygons have no neighbors (indicated by the value 0), while the sixth polygon has a neighbor corresponding to index 35204.

With "table(sapply(nb_0, length))" we get 40279 polygons without neighbors, 3630 polygons with one neighbor, and so on.

*Neighbor Frequency*

42,131 polygons have one neighbor. This indicates that most of the polygons in your dataset are neighbors of only one other polygon, which is important information to consider in spatial analysis.

984 polygons have two neighbors, which is also significant, although it represents a smaller number compared to those with only one neighbor.

*Group Identification*

The existence of polygons with more than two neighbors (up to 29 at the maximum) suggests that there are certain clusters or groupings of polygons that are more closely connected to each other. This can be useful for spatial analysis, as it can indicate areas where there is greater interconnectedness.

**Prompt:**

¿How can we apply Spatial Neighborhood Matrices?

The neighbor matrix will aid in building spatial models that analyze the correlation and dependence between data in different geographic areas.

Diagnosing Spatial Effects: By evaluating how the characteristics of a polygon may affect its neighbors, you can obtain richer and more relevant inferences about your dataset.

------------------------------------------------------------------------

## Neighbors Based on k Nearest Neighbors

**Valle del Cauca:**

```{r}
#| echo: true
#| message: false
#| warning: false

# Compute centroids of the polygons
coo <- st_centroid(delitos_data)

# Create a neighbor list where each polygon (based on its centroid `coo`) is connected 
# to its 5 nearest neighbors using k-nearest neighbors (k = 5)
nb <- knn2nb(knearneigh(coo, k = 5)) # k number nearest neighbors

# Polygons with neighbors
table(sapply(nb, length))

# Subset data to the first 10 polygons
delitos_data_10 <- delitos_data[1:1000, ]

# Recompute neighbor list for these 1000 polygons to avoid index mismatches
nb_10 <- knn2nb(knearneigh(st_centroid(delitos_data_10), k = 5))

# Compute centroids for the 1000 polygons
coords_10 <- st_coordinates(st_centroid(delitos_data_10))

# Plot the first 1000 polygons and overlay neighbor connections in red
plot(st_geometry(delitos_data_10), border = "lightgray", main = "First Polygons with 5 Nearest Neighbors")
plot.nb(nb_10, coords_10, add = TRUE, col = "red", lwd = 2)
```

**Centroid calculation**

coo \<- st_centroid(delitos_data)

Here first step is to compute all measurements of **centroids** of the polygons in the crimes_data object. Each polygon (possibly representing a geographic area where crimes occurred) will have a center point calculated as its "center of gravity."

**Creating the neighbor list**

nb \<- knn2nb(knearneigh(coo, k = 5))

Then, a neighbor list is created using the k-nearest neighbors method (k = 5). This means that for each polygon, its three closest polygons (based on the distance between their centroids) are found.

**Summary of the number of neighbors**

table(sapply(nb, length))

This command generates a table showing how many polygons have 5 neighbors. The result shows that there are 45,328 polygons in total, all of which have 5 neighbors, indicating that the neighbor calculation was successful for this dataset.

**Data subset:**

Subset data to the first 1000 polygons

delitos_data_10 \<- delitos_data\[1:1000, \]

Here a subset containing only the first 1000 polygons of the original dataset is created for further analysis.

**Recalculating the neighbor list for the subset**

nb_10 \<- knn2nb(knearneigh(st_centroid(delitos_data_10), k = 5))

Se vuelve a calcular la lista de vecinos, pero esta vez solo para los 1000 polígonos en el subconjunto **`delitos_data_10`**.

**Calculation of centroid coordinates for the subset**

coords_10 \<- st_coordinates(st_centroid(delitos_data_10))

Obtiene las coordenadas de los centroides de los 1000 polígonos seleccionados.

**Display**

plot(st_geometry(delitos_data_10), border = "lightgray", main = "First Polygons with 5 Nearest Neighbors") plot.nb(nb_10, coords_10, add = TRUE, col = "red", lwd = 2)

Here, in summary, we analyze a set of polygons that could represent geographic areas of interest, such as regions where crimes are reported. First, the centroids of all these polygons are calculated using the st_centroid function on the crimes_data set. Then, a list of neighborhoods is generated using the k-nearest neighbors method with k = 5, meaning that each polygon is connected to its five nearest neighbors based on the distance between their centroids. The result, table(sapply(nb, length)) confirms that each of the 45,328 polygons in Valle del Cauca has exactly 5 neighbors. Subsequently, a subset of the first 1,000 polygons is selected for further visual analysis. For these, the neighbor list is recalculated again to ensure there are no index mismatches. Finally, a graph is generated plotting these 1,000 polygons, with neighborhood links represented by red lines between neighboring centroids. This analysis shows how each area in the set is geographically connected to its immediate surroundings, helping to understand the distribution and spatial relationships of the phenomenon represented by the crime data.

![KNN](https://github.com/Alonso666cr/Advanced_Data_Analysis/blob/main/KNN.png?raw=true){fig-align="center" width="365"}

This type of analysis is useful for exploring the spatial relationship between areas where crimes occurred, identifying patterns that may be related to geographic proximity.

------------------------------------------------------------------------

## Neighbors Based on Distance

```{r}
#| echo: true
#| message: false
#| warning: false

# Create a neighbor list using distance-based contiguity:
# Polygons are considered neighbors if their centroids are within 0.4 units (e.g., degrees) apart
nb <- dnearneigh(x = st_centroid(delitos_data), d1 = 0, d2 = 0.4)

# Polygons with neighbors
# Calcular el número de vecinos
vecinos <- sapply(nb, length)

# Crear el histograma con 20 columnas
# Calcular el número de vecinos
vecinos <- sapply(nb, length)

# Crear el histograma con 20 columnas pero con un rango ajustado
hist_info <- hist(vecinos, 
                   breaks = seq(min(vecinos) - 1, max(vecinos) + 1, length.out = 21),  # Ajustando los límites
                   main = "Número de Vecinos por Polígono", 
                   xlab = "Número de Vecinos", 
                   ylab = "Frecuencia", 
                   col = "lightblue", 
                   border = "black")

# Agregar los números de polígonos (frecuencia) sobre cada barra
text(hist_info$mids, hist_info$counts, 
     labels = hist_info$counts, 
     pos = 3,          
     cex = 0.7)


 
#---library(openxlsx)
# Calcular el número de vecinos
#---vecinos <- sapply(nb, length)
# Crear un data frame con los resultados
#---df_vecinos <- data.frame(vecis = vecinos)
# Escribir el data frame a un archivo Excel
#---write.xlsx(df_vecinos, file = "Vecinos_Distancia.xlsx")

# Subset data to the first 10 polygons
delitos_data_10 <- delitos_data[1:100, ]

# Recompute neighbor list for these 10 polygons to avoid index mismatches
nb_10 <- dnearneigh(x = st_centroid(delitos_data_10), d1 = 0, d2 = 0.4)

# Compute centroids for the 10 polygons
coords_10 <- st_coordinates(st_centroid(delitos_data_10))

# Plot the first 10 polygons and overlay neighbor connections in red
plot(st_geometry(delitos_data_10), border = "lightgray", main = "First Polygons with 3 Nearest Neighbors")
plot.nb(nb_10, coords_10, add = TRUE, col = "red", lwd = 2)
```

**Valle del Cauca:**

```{r}
# Comprobar el sistema de referencia de coordenadas
crs_info <- st_crs(delitos_data)
print(crs_info)
```

In this approach, the neighbors of a datum (or point) are determined based on a fixed distance criterion. Any point within a specified radius or distance from a reference point is considered a neighbor.

We have a histogram that groups the number of polygons by neighbor frequency. For example, in the first column, there are 3,711 polygons with up to 10 neighbors.

This histogram allows us to visualize how the polygons are distributed based on their number of neighbors, which can be useful for analyzing connectivity and the spatial relationships between them.

As in KNN, the centroids of the polygons are calculated and a list of neighbors is created, considering that two polygons are neighbors if the distance between their centroids is within the defined range, in this case it is 0.4 units and after doing the investigation, it is determined that it is approximately 44 kilometers.

## Neighborhood Matrices

```{r}
#| echo: true
#| message: false
#| warning: false

# Spatial weights matrix using Queen contiguity (binary weights)
# 'queen = TRUE' considers shared edges OR vertices as neighbors
nb <- poly2nb(delitos_data, queen = TRUE)

# Convert the neighbor list to a spatial weights list object
# 'style = "W"' row-standardizes the weights (sums to 1)
# 'zero.policy = TRUE' avoids errors when some polygons have no neighbors
nbw <- spdep::nb2listw(nb, style = "W", zero.policy = TRUE)

# Display the first 10 rows of spatial weights (for the first 10 polygons)
nbw$weights[1:10]

# Spatial weights matrix based on inverse distance values
# Compute centroids of polygons
coo <- st_centroid(delitos_data)

# Use Queen contiguity again to define neighbors
nb <- poly2nb(delitos_data, queen = TRUE)

# Compute distances between neighbors based on their centroids
dists <- nbdists(nb, coo)

# Create inverse distance weights (1/distance) for each pair of neighbors
ids <- lapply(dists, function(x){1/x})

# Create a listw object using binary style ("B" = no standardization)
nbw <- nb2listw(nb, glist = ids, style = "B", zero.policy = TRUE)

# Display the first 10 inverse-distance-based weights
nbw$weights[1:10]
```

We begin again by calculating the polygon centroids and, with that, calculating their distances.

Here, the contiguity type used to define neighbors is Queen.

The distances between the centroids of the neighboring polygons are calculated. This generates a list where each element corresponds to a polygon and contains the distances to its neighbors.

Then, the inverse of each distance (1/distance) is calculated to obtain the weighting, with the closest polygons having a higher weight.

Next, a list-like object is obtained with the neighbors and the inverse of the weights, and then the first 10 positions are displayed.

This is where the questions started!

**Prompt:**

What does the NULL value represent here?

The NULL output indicates that, for those specific indices, there are no assigned neighbors, or the distances are so large that no weights are generated for those polygons. It's normal if some polygons don't have neighbors according to the defined contiguity.

**Prompt:**

What do the numbers next to some of these indices represent?

For the indices that show values ​​(for example, \[\[6\]\] and \[\[9\]\]), the inverse distance weights are observed:

\[\[6\]\] has a weight of approximately 0.2475635.

\[\[9\]\] shows several weights that are slightly lower, indicating the proximity relationships between the corresponding polygon and its neighbors.

Continuing with the learning, I understood that, for example, in the case of:

\[\[6\]\] \[1\] 0.2475635

That 6 is the position of a polygon in my dataset delitos_data. But I wanted to know the name of the locality represented by that 6th position.

```{r}
# Averiguar el número de polígonos:
n_polygons <- nrow(delitos_data)
print(paste("Polygons in delitos_data: ", n_polygons))

# Consultar el Polígono correspondiente
poligono_seis <- delitos_data[6, ]  # Esto te dará el polígono en la fila 6
print("This is the polygon in 6th position:")
print(poligono_seis)
# dpto_ccdgo y mpio_ccdgo muestran el departamento y municipio que corresponde al poligono.
# En este caso dpto_ccdgo[76] y mpio_ccdgo[147] => Corresponde a Cartago.

# Ver el Contenido de los Vecinos
# Conocer cuántos índices de los polígonos vecinos del poligono "Cartago" (posición 6).
# Obtener los vecinos del polígono en la posición 6
vecinos_seis <- nb[[6]]
print(vecinos_seis)
```

Using:

poligono_seis \<- delitos_data\[6, \] \# Esto te dará el polígono en la fila 6

print("This is the polygon in 6th position:")

print(poligono_seis)

I can find which location is mpio_cdpmp =\> 76147 using this next chunk, this is mine!

```{r}
# install library
#install.packages("readxl")

# Load package
library(readxl)

# Read Excel Book
Municipios <- read_excel("municipios.xlsx")

# Verifica los primeros registros para asegurarte de que se ha cargado correctamente
print(head(Municipios))

# Querying NOMBRE_MPIO ysing CODIGO_MPNIO
#install.packages("dplyr")
library(dplyr)

# Supongamos que deseas buscar el nombre del municipio con CODIGO_MPIO 91405
codigo_a_buscar <- "76147"

# Usar dplyr para filtrar y seleccionar el nombre del municipio
nombre_municipio <- Municipios %>%
  filter(CODIGO_MPIO == codigo_a_buscar) %>%
  select(NOMBRE_MPIO)

# Mostrar el resultado
print(nombre_municipio)
```

# Spatial autocorrelation {.unnumbered}

## Global Moran’s Index

It is one of the main methods in spatial analysis to detect patterns of clustering (correlation) or dispersion of similar values ​​in a geographic space.

° A value close to +1 indicates strong positive spatial autocorrelation (areas with high values ​​are close to areas with high values, and those with low values ​​are close to areas with low values).

° A value close to 0 indicates no spatial autocorrelation (values ​​are randomly distributed).

° A value close to -1 indicates negative autocorrelation (areas with high values ​​tend to be close to areas with low values).

```{r}
library(spdep)

# Compute centroids of the polygons
coo <- st_centroid(delitos_data)

# Create a neighbor list where each polygon (based on its centroid `coo`) is connected 
# to its 3 nearest neighbors using k-nearest neighbors (k = 3)
nb <- knn2nb(knearneigh(coo, k = 3)) # k number nearest neighbors

# Global Moran's I
# Convert the neighbor list to a listw object
lw <- nb2listw(nb, style = "W") # Use nb2listw

# Now you can use 'lw' in moran.test
gmoran <- moran.test(delitos_data$sum_24HP, lw, alternative = "greater")

gmoran

gmoran[["estimate"]][["Moran I statistic"]] # Moran's I

gmoran[["statistic"]] # z-score

gmoran[["p.value"]] # p-value
```

**Prompt:**

What is its importance in spatial analysis?

**° Detecting spatial patterns:** It allows you to identify whether phenomena (such as crime, poverty, disease, etc.) tend to concentrate or disperse in specific areas.

**° Decision-making:** It aids in urban planning, public health, economics, etc., by understanding the spatial structure of the data.

**° Application in modeling:** Its value can help decide whether spatial models (such as spatial regression models or cluster analysis) are required.

It is essential in spatial analysis because it helps detect global patterns and decide whether it is necessary to apply models that consider spatial structure.

**Process explanation**

Thus, by applying the Global Moran's Index, we can assess whether crimes are spatially clustered in crimes_data. Using the centroids, we calculate the nearest neighbors using the KNN with K=3 algorithm based on the Euclidean distance between centroids. This gives a list of neighbors, which is then used to apply the Moran's index and verify the presence of spatial autocorrelation in sum_24HP. This index is calculated by converting the list of neighbors into a spatial weights matrix and applying the Moran's statistical test to determine if there is significant spatial autocorrelation in the variable sum_24HP. The variable sum_24HP represents counts or sums in each polygon (e.g., number of crimes in each unit), and this analysis assesses whether these counts are spatially clustered.

**Result Summary**

Moran I value: ***0.1752345*** This positive value indicates that there is a positive spatial autocorrelation in your data, which implies that polygons with similar values ​​in sum_24HP tend to be close to each other, forming groups or clusters.

Z-score (standard statistics): 49.16138 It is an extremely high value, which indicates that the observed autocorrelation is very different from what would be expected if there was no autocorrelation (low null hypothesis).

P-value: \< 2.2e-16 (practically zero) This means that the probability of the observed autocorrelation pattern occurring by chance is **practically zero**.

**Conclusion**

There is a statistically significant spatial autocorrelation in your data (sum_24HP). That is, crime rates in one area are related to rates in nearby areas.

The magnitude of Moran's I (0.1752) suggests a moderate positive autocorrelation, but in spatial analyses, values ​​between 0.1 and 0.3 are generally considered moderate autocorrelation.

## Local Moran’s Index

```{r}
#| echo: true
#| message: false
#| warning: false

lmoran <- localmoran(delitos_data$sum_24HP, lw, alternative = "greater")
head(lmoran)
```

Commonly called LISA, it is an extension of the Global Moran Index, but instead of giving a single value for the entire sample, it provides a measure of spatial autocorrelation for each individual spatial unit, in this case for each polygon.

**Prompt:**

What is it for?

It allows you to identify specific “clusters” on the map, such as areas with high values ​​surrounded by high values ​​(hot spots) or low values ​​surrounded by low values ​​(cold spots).

In this case, the variable of interest "sum_24HP" is evaluated for each spatial unit.

It is important to clarify that the value of the local Moran index, for each polygon, is in the first column (Ii) of each record in the previous table.

The remaining columns (E.Ii, Var.Ii, Z.Ii, Pr(z \> E(Ii))) provide additional information to evaluate the significance of the Ii value.

**Prompt:**

How are the local Moran index values ​​interpreted?

| **C**olumn | **D**escription | Interpretation |
|:-----------------:|:------------------------:|:-------------------------:|
| **Ii** | The value of the **Local Moran Index** (autocorrelation value for that spatial unit). | The larger and positive the hotspot, the stronger the autocorrelation; the negative indicates low clustering. |
| **E.Ii** | The expectation (expected value) under the hypothesis of absence of autocorrelation. | For Moran, it is usually close to 0. |
| **Var.Ii** | The **variance** of the local index under the null hypothesis. | Mide la dispersión esperada si no hay autocorrelación espacial real. |
| **Z.Ii** | Standard **Z statistic** (normalized value). | If it is very high or very low (in absolute value), it indicates that the index value is significantly different from 0. |
| **Pr(z \> E(Ii))** | **P-value** for the test (that the index is greater than the expected value, in this case, of positive autocorrelation). | If it is small (e.g. \< 0.05), the positive autocorrelation in that unit is significant. |

**What does this mean in practice?**

Rows with **high Z values ​​and small p values** ​​indicate units where there is **significant spatial autocorrelation** — that is, those areas are **hotspots** (highs surrounded by highs).

-   In this example, the first rows (1 to 4) have a positive **`Ii`** and p-values ​​around **`0.35`**, **they are not statistically significant** in positive autocorrelation.

-   Row 5 has a negative **`Ii`** and a p-value around **`0.54`**, which indicates **no significant autocorrelation** either positive or negative in that unit.

-   The consistency in the values ​​of **E.Ii** (~~0), de **Var.Ii** (~~0.017), and the high p-values ​​in most of the rows, suggests that in those units **there is no significant autocorrelation** in the data.

-   To detect **hotspots** or **clusters** on your map, focus on the rows where **p-valor \< 0.05** y **Z-Ii** is high (positive) — these are the areas with significant autocorrelation.

**Differences between Moran's Global Index and Local Index (LISA)**

| Global Moran's Index | Local Moran's Index (LISA) |
|--------------------------------|:---------------------------------------|
| Measures the overall pattern across the entire dataset. | Detects specific spatial patterns in particular areas or groups on the map. |
| Provides a single value for the whole region. | Provides a value for each area or spatial unit. |
| Useful to determine if there is overall autocorrelation. | Useful to identify specific positive or negative clusters (e.g., areas with high crime surrounded by other high-crime areas). |
| Example: "Is there spatial autocorrelation across the city?" | Example: "Where are the high-crime clusters?" |

## Clusters

Remember that the **Global Moran Index** summarizes the total degree of spatial autocorrelation in the entire data sample, in this case crimes in each polygon.

I had to research how to interpret clusters in a Moran diagram.

Reference: <http://scielo.sld.cu/scielo.php?script=sci_arttext&pid=S1817-40782014000200001>

The Moran diagram is obtained from the matrix expression derived from the Global Moran test. The Global Moran test is based on the slope of the line, such that the greater the angle it forms with the abscissa axis, the greater the degree of autocorrelation. Figure 1 is shown below as an example.

![](https://github.com/Alonso666cr/Advanced_Data_Analysis/blob/main/Diagrama_Moran1.jpg?raw=true){style="width: 50%; height: 50%;" fig-align="center"}

*From this diagram, the phenomenon is divided into four quadrants: the first and third quadrants (High-High; Low-Low) are indicative of positive autocorrelation, which means that the behavior of a variable in some regions is being favored by its behavior in nearby regions. On the other hand, the second and fourth quadrants (Low-High; High-Low) reflect the existence of negative spatial autocorrelation, meaning that the behavior of a variable is producing contrary effects in nearby regions.*

*The first case (A-A; B-B) indicates the presence of high values ​​of the variable surrounded by high values, or low values ​​surrounded by low values. The second case (B-A; A-B) means the presence of low values ​​of the variable surrounded by high values, or vice versa. In this particular case, the presence of positive spatial autocorrelation can be detected, observing the positive slope of the Moran test, which reaches a significance value of approximately 36%.*

With this reference, I now understand that the diagram is made up of 4 quadrants.

-   The line that runs through the Moran diagram is, in fact, the theoretical regression line under the hypothesis of spatial independence (null hypothesis).

-   The hypothesis of spatial independence is a statistical assumption that states that the values ​​of the variable at each point or spatial unit are independent of the values ​​in the other units.

-   In other words, the value at one location does not depend on the values ​​at nearby locations, that is, the null hypothesis is assumed and to determine whether the data contradict that hypothesis, a statistical test such as Moran's is performed.

-   This hypothesis is the basis for assessing whether there is autocorrelation: if it can be shown that the values ​​are not spatially related, the independence hypothesis is accepted.

    **If the test shows that the null hypothesis is rejected:**

    It is because **there is spatial autocorrelation**, that is, the values ​​in nearby areas tend to be significantly similar or dissimilar..

-   **If not rejected:**\
    The distribution of values ​​can be considered random, with no evidence of autocorrelation.

-   When the hypothesis is rejected (the data show that there is a relationship), then there is spatial autocorrelation, which means that values ​​in nearby locations tend to be similar (or in some cases, dissimilar).

<!-- -->

-   It is a line that attempts to represent the expected relationship between the values ​​of the variable in an area and the weighted average value of its neighbors.

-   In this analysis, that line corresponds to the linear regression of the values ​​of the variable as a function of the values ​​of its neighbors, or to a line that maximizes the linear relationship between these two variables.

-   **The slope** of that line measures **the degree of spatial autocorrelation**: higher slopes indicate greater positive autocorrelation and vice versa.

-   The value of the slope of the regression line in the diagram is precisely the Global Moran Index.

The following code I built with AI and it gives me the angle in degrees: 9.939283 of the regression line with respect to the X axis.

```{r}
# 0.1752345

library(sf)
library(spdep)

# 1. Calcular centroides
coo <- st_centroid(delitos_data)

# 2. Encontrar los vecinos k-vecinos más cercanos
knn_result <- knearneigh(coo, k = 3)

# 3. Convertir a lista de vecinos
nb <- knn2nb(knn_result)

# 4. Convertir a matriz de pesos
lw_mat <- nb2mat(nb, style = "W")

# 5. Datos estandarizados
datos_std <- as.vector(scale(delitos_data$sum_24HP))

# 6. Calcular wx
wx <- as.vector(lw_mat %*% datos_std)

# 7. Ajuste la regresión lineal
regresion_df <- data.frame(x = datos_std, wx = wx)
modelo <- lm(wx ~ x, data = regresion_df)
summary(modelo)

# 8. Pendiente y ángulo
pendiente <- coef(modelo)[2]
cat("Pendiente:", pendiente, "\n")
angulo_rad <- atan(pendiente)
cat("Ángulo en grados:", angulo_rad * (180 / pi), "\n")
```

![](https://github.com/Alonso666cr/Advanced_Data_Analysis/blob/main/Diagrama_Moran2.png?raw=true){style="width: 50%; height: 50%;" fig-align="center"}

Here we have the corresponding Moran diagram, which shows the relationship between each value of the variable SUM_24HP and the weighted average value of its neighbors, helping to visualize whether there is spatial autocorrelation in the entire data set. That is, it is a graphical representation for analyzing global autocorrelation.

```{r}
#| echo: true
#| message: false
#| warning: false

mp <- moran.plot(as.vector(scale(delitos_data$sum_24HP)), lw)
delitos_data$quadrant <- NA
# high-high
delitos_data[(mp$x >= 0 & mp$wx >= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 1
# low-low
delitos_data[(mp$x <= 0 & mp$wx <= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 2
# high-low
delitos_data[(mp$x >= 0 & mp$wx <= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 3
# low-high
delitos_data[(mp$x <= 0 & mp$wx >= 0) & (delitos_data$lmp <= 0.05), "quadrant"]<- 4
# non-significant
delitos_data[(delitos_data$lmp > 0.05), "quadrant"] <- 5
```

**Prompt:**

Can you help me interpret the results? That is, could we determine if there's any spatial clustering?

**Diagram interpretation:**

-   Points farther from the center (in the High-Low and Low-High quadrants) represent potential spatial outliers, or areas with values ​​that do not follow the general pattern.

-   There are scattered points in quadrants indicating High-High (top right) and Low-Low (bottom left) Clusters, which are clear indications of positive spatial autocorrelation, consistent with the high Moran's I value.

-   Most of the points are clustered on the left side of the graph, close to 0 on the X-axis and at the bottom of the Y-axis, indicating many areas with moderate or low crime rates and their neighborhoods also with low or moderate rates.

-   The dispersion in the high-high and low-low quadrants indicates the presence of spatial clusters of crime, which may be important for targeting interventions.

-   It's important to note that the Global Moran's Index on which the diagram is based was calculated using KNN=3 as the algorithm for calculating neighbor contiguity. The slope of the trend in that graph, and the classification into quadrants, reflect the degree of spatial autocorrelation, which, in your analysis, is represented by the Moran's I value of 0.1752.

-   **The plot** and **the value (0.1752)** work together to indicate that there is a moderate positive autocorrelation in your data with the neighbor structure defined by k=3.

In summary:

```         
Moran I statistic standard deviate = 49.161, p-value < 2.2e-16
```

-   **The p-value** is extremely small (much less than 0.05), indicating that **the null hypothesis of spatial independence is rejected.**

-   This means that, **with high confidence**, there is statistical evidence to affirm that **there is positive spatial autocorrelation** in your data.

-   There is significant spatial autocorrelation.

```{r}
library(sf)
library(spdep)

# 1. Asegúrate que tus datos espaciales `delitos_data` sean un sf con geometrías de polígonos

# 2. Crear los centroides (si no lo hiciste)
coo <- st_centroid(delitos_data)

# 3. Crear la estructura de vecinos K=3 (ajusta k si quieres)
knn_result <- knearneigh(coo, k=3)
nb <- knn2nb(knn_result)

# 4. Crear matriz de pesos
lw_mat <- nb2mat(nb, style = "W")

# 5. Variables de interés: `sum_24HP`
datos_var <- delitos_data$sum_24HP

# 6. Valores estandarizados
datos_std <- as.vector(scale(datos_var))

# 7. Valor ponderado (wx)
wx_vals <- as.vector(lw_mat %*% datos_std)

# 8. Test de Moran para obtener p-valor
moran_test <- moran.test(datos_var, lw)

p_value <- moran_test$p.value

# 9. Crear variable 'quadrant' basada en los valores y p-valor
delitos_data$quadrant <- NA

# Clasificación en cuadrantes según análisis espacial
delitos_data[(datos_std >= 0 & wx_vals >= 0) & (p_value <= 0.05), "quadrant"] <- 1  # High-High
delitos_data[(datos_std <= 0 & wx_vals <= 0) & (p_value <= 0.05), "quadrant"] <- 2  # Low-Low
delitos_data[(datos_std >= 0 & wx_vals <= 0) & (p_value <= 0.05), "quadrant"] <- 3  # High-Low
delitos_data[(datos_std <= 0 & wx_vals >= 0) & (p_value <= 0.05), "quadrant"] <- 4  # Low-High
delitos_data[p_value > 0.05, "quadrant"] <- 5  # No significativa

# 10. Mapear los resultados con tmap

library(tmap)

tm_shape(delitos_data) + 
  tm_fill(
    col = "quadrant",
    title = "",
    breaks = c(1, 2, 3, 4, 5, 6),
    palette = c("red", "blue", "lightpink", "skyblue2", "white"),
    labels = c("High-High", "Low-Low", "High-Low", "Low-High", "Non-significant")
  ) +
  tm_borders(alpha = 0.5) +
  tm_layout(frame = FALSE, title = "Clusters") +
  tm_legend(outside = TRUE, text.size = 1)

```

![](https://github.com/Alonso666cr/Advanced_Data_Analysis/blob/main/Mapa_de_Valle_del_Cauca.jpg?raw=true){fig-align="center" width="60%"}

![](https://github.com/Alonso666cr/Advanced_Data_Analysis/blob/main/Mapa_de_Valle_del_Cauca.jpg?raw=true){fig-align="center" width="60%"}

```{r}
#| echo: true
#| message: false
#| warning: false

tm_shape(delitos_data) + tm_fill(col = "quadrant", title = "",
breaks = c(1, 2, 3, 4, 5, 6),
palette =  c("red", "blue", "lightpink", "skyblue2", "white"),
labels = c("High-High", "Low-Low", "High-Low",
           "Low-High", "Non-significant")) +
tm_legend(text.size = 1)  + tm_borders(alpha = 0.5) +
tm_layout(frame = FALSE,  title = "Clusters")  +
tm_layout(legend.outside = TRUE)
```

I didn´t know the name for this kind of map.

**Prompt:**

What kind of map is this ?

A choropleth map is a type of thematic map that uses differences in color or fill pattern to show how the values ​​of a variable are distributed across different geographic areas. These areas are typically administrative divisions such as municipalities, counties, or districts.

In this case R takes "tmap" library to visualize spacial data, specifically to show spacial autocorrelation clusters in "delitos_data" dataset.

El mapa pinta las zonas en base a su **ubicación geográfica y su clasificación en clústeres espaciales**, considerando sus coordenadas y relaciones con las áreas cercanas, según el análisis de autocorrelación definido.

Se basa en las coordenadas geográficas de **cada polígono**, porque cada polígono representa una área territorial

La categorización (el valor en **`"quadrant"`**) indica **en qué clúster o patrón espacial** se encuentra esa área, según el análisis de autocorrelación espacial realizado antes para el diagrama de Morán.

No es simplemente un mapa de las variables originales (como número de delitos) sino un mapa que muestra **dónde están esos patrones de agrupamiento espacial**.

```{r}

# Paso 1: Importar el archivo KML en R --------------------------------------
library(sf)

# Ruta del archivo KML
archivo_kml <- "VALLE DEL CAUCA.kml"

# Leer el KML usando sf
region_sf <- st_read(archivo_kml, quiet = TRUE)


# Paso 2: Verificar y transformar la proyección (CRS) -----------------------
# Revisar CRS del mapa principal
st_crs(delitos_data)

# Revisar CRS del archivo importado
st_crs(region_sf)

# Si no coinciden, proyectar uno a la misma referencia del otro
region_sf <- st_transform(region_sf, crs = st_crs(delitos_data))


# Paso 3: Visualizar ambas capas juntos --------------------------------------
###library(tmap)

# Mapa combinado
###tm_shape(delitos_data) + 
  ###tm_fill(col = "quadrant") +  # tu mapa de clústeres
  ###tm_shape(region_sf) + 
  ###tm_borders(col = "black", lwd = 2)

library(ggplot2)

ggplot() +
  geom_sf(data = delitos_data, aes(fill = factor(quadrant)), color = "black") +
  geom_sf(data = region_sf, fill = NA, color = "red", lwd = 1)

```

```{r}
library(sf)
library(raster)
library(tmap)
library(magick)

# Leer el KML
valle_kml <- st_read("VALLE DEL CAUCA.kml")  # reemplaza con tu archivo

# Obtener el bounding box
bbox <- st_bbox(valle_kml)
xmin <- bbox["xmin"]
xmax <- bbox["xmax"]
ymin <- bbox["ymin"]
ymax <- bbox["ymax"]

# Leer y hacer flip vertical (invertir la imagen en eje y)
img_magick <- image_read("Mapa_de_Valle_del_Cauca.jpg")
img_flip <- image_flip(img_magick)  # hace un flip vertical

# Guardar la imagen invertida en archivo temporal
image_write(img_flip, "Mapa_flip.png")

# Cargar como raster
img <- raster("Mapa_flip.png")

# Asignar extensión y CRS
extent(img) <- extent(xmin, xmax, ymin, ymax)
crs(img) <- st_crs(valle_kml)$proj4string

# Mapear y superponer
tm_shape(st_as_sf(valle_kml)) +
  tm_borders() +
  tm_shape(img) +
  tm_raster(alpha = 0.5)


```

```{r}
library(sf)
library(raster)
library(tmap)
library(magick)

# 1. Leer el archivo KML del Valle del Cauca
valle_kml <- st_read("VALLE DEL CAUCA.kml")

# 2. Obtener el bounding box para extensión
bbox <- st_bbox(valle_kml)
xmin <- bbox["xmin"]
xmax <- bbox["xmax"]
ymin <- bbox["ymin"]
ymax <- bbox["ymax"]

# 3. Leer y girar la imagen
img_magick <- image_read("Mapa_de_Valle_del_Cauca.jpg")
#img_flip <- image_flip(img_magick)  # flip vertical
img_flip <- img_magick  # without flip vertical
image_write(img_flip, "Mapa_flip.png")  # guardar temporalmente

# 4. Cargar la imagen como raster
library(raster)
img_raster <- raster("Mapa_flip.png")
extent(img_raster) <- extent(xmin, xmax, ymin, ymax)
crs(img_raster) <- st_crs(valle_kml)$proj4string

# 5. Convertir raster a objeto 'stars' para tmap
library(stars)
img_stars <- st_as_stars(img_raster)

# 6. Preparar tus puntos de delitos (si no tienes sf, crea uno)
# Aquí asumo que tienes un data.frame 'delitos' con columnas 'lon' y 'lat'
# Si ya tienes sf, obvia este paso
# delitos_sf <- st_as_sf(delitos, coords = c("lon", "lat"), crs = 4326)

# Si ya tienes los puntos en sf, reemplaza la línea siguiente por tu variable:
# Aquí creamos un ejemplo ficticio (elimina o ajusta según tu data real)
# delitos_sf <- st_read("tus_puntos_delitos.shp") # si ya tienes un shapefile
# O si tienes coordenadas en df:
# delitos_sf <- st_as_sf(delitos, coords = c("lon", "lat"), crs = 4326)

# Crear capa sf a partir del data frame con coordenadas
delitos_sf <- st_as_sf(delitos_data, coords = c("lon", "lat"), crs = 4326)

# ---- IMPORTANTE ----
# Asegúrate de que 'delitos_sf' esté cargado y tenga CRS compatible con el mapa

# 7. Mapear todo en tmap
tm_shape(st_as_sf(valle_kml)) +
  tm_borders() +
  tm_shape(img_stars) +
  tm_raster(alpha = 0.5) +
  # Agregar los puntos grandes y en negro
  tm_shape(delitos_sf) +
  tm_bubbles(size = 0.3, col = "black") +  # ajusta tamaño aquí
  tm_layout(legend.outside = TRUE)

```

# Correspondance Analysis {.unnumbered}

```{r}
#| eval: false
#| message: false
#| warning: false
#| include: false

# Paso 1: Crear la matriz de frecuencias de delitos en polígonos
Freq <- delitos_data %>%
  st_drop_geometry() %>%
  select(contains('24'))

rownames(Freq) <- c("Zona Norte", "Zona Sur", "Zona Este", "Zona Oeste")
colnames(Freq) <- c("Homicidio", "Robo", "Hurto", "Extorsión")

print("Matriz de frecuencias:")
print(Freq)

# Paso 2: Calcular la tabla de frecuencias relativas
total_F <- sum(Freq)
F_rel <- F / total_F

print("Matriz de frecuencias relativas:")
print(F_rel)

# Paso 3: Cálculo de los totales por fila y columna
fi <- rowSums(F_rel)  # Suma por fila
fj <- colSums(F_rel)  # Suma por columna

D_f <- diag(fi)  # Matriz diagonal de totales de fila
D_c <- diag(fj)  # Matriz diagonal de totales de columna

# Paso 4: Construcción de la matriz de correspondencias Z
D_f_sqrt_inv <- diag(1 / sqrt(fi))  # Raíz cuadrada inversa de los totales de fila
D_c_sqrt_inv <- diag(1 / sqrt(fj))  # Raíz cuadrada inversa de los totales de columna

Z <- D_f_sqrt_inv %*% (F_rel - fi %o% fj) %*% D_c_sqrt_inv

print("Matriz de correspondencias Z:")
print(Z)

# Paso 5: Descomposición en Valores Singulares (SVD)
svd_result <- svd(Z)  # Descomposición de Z

U <- svd_result$u  # Vectores propios de ZZ' (Filas)
D <- diag(svd_result$d)  # Matriz diagonal de valores singulares
V <- svd_result$v  # Vectores propios de Z'Z (Columnas)

print("Matriz U (Vectores propios de ZZ'):")
print(U)

print("Matriz D (Valores singulares):")
print(D)

print("Matriz V (Vectores propios de Z'Z):")
print(V)

# Paso 6: Proyección de filas y columnas en el espacio reducido
dim_reducida <- 2
C_f <- D_f_sqrt_inv %*% U[, 1:dim_reducida] %*% D[1:dim_reducida, 1:dim_reducida]  # Coordenadas filas
C_c <- D_c_sqrt_inv %*% V[, 1:dim_reducida] %*% D[1:dim_reducida, 1:dim_reducida]  # Coordenadas columnas

print("Coordenadas filas en el espacio reducido:")
print(C_f)

print("Coordenadas columnas en el espacio reducido:")
print(C_c)

# Librerías
library(MASS)  # Para manipulación matricial
library(ca)  # Para el análisis de correspondencias

# Matriz de frecuencias (ejemplo: delitos en polígonos)
F <- matrix(c(
  12, 5, 8, 2,
  7, 3, 10, 1,
  4, 2, 6, 1,
  15, 8, 14, 5
), nrow = 4, byrow = TRUE)

rownames(F) <- c("Zona Norte", "Zona Sur", "Zona Este", "Zona Oeste")
colnames(F) <- c("Homicidio", "Robo", "Hurto", "Extorsión")

# Total de la tabla
n_total <- sum(F)

# Matriz de frecuencias relativas
F_rel <- F / n_total

# Totales por fila y columna
f_i <- rowSums(F_rel)
f_j <- colSums(F_rel)

# Matriz esperada bajo independencia
F_exp <- outer(f_i, f_j)

# Cálculo de ji-cuadrado
chi2 <- sum((F_rel - F_exp)^2 / F_exp) * n_total
print(paste("Estadístico Ji-cuadrado:", round(chi2, 4)))

# Distancia ji-cuadrado entre filas
D_c_inv <- diag(1 / f_j)  # Inversa de los totales de columnas
dist_ji <- as.matrix((F_rel - F_exp) %*% D_c_inv %*% t(F_rel - F_exp))
print("Matriz de distancias ji-cuadrado entre filas:")
print(dist_ji)
```

Here I found this error:

![](https://github.com/Alonso666cr/Advanced_Data_Analysis/blob/main/Error_R_Correspondance_Analysis.png?raw=true){width="389"}

I have no idea about why could I expect running this chunk.\

# Imputing and Linking Data

## Download open spatial data

```{r}
#| echo: true
#| message: true
#| warning: true

d <- worldclim_country(country = "Colombia", var = "tmin",
                       path = tempdir())
terra::plot(mean(d), plg = list(title = "Min. temperature (C)"))
```

**Prompt:**

**What is WorldClim?**

WorldClim is a global database that provides historical, current, and future climate data at high spatial resolutions.

**What type of data does it offer?**

-   Main climate variables: maximum, minimum, and average temperature, precipitation, humidity, among others.

-   Spatial resolution: typically 1 km to 5 km, depending on the dataset.

-   Time periods: historical (e.g., 1970–2000), current climate, and future projections based on climate models.

**What is it used for?**

-   Biodiversity and ecological studies.

-   Land use planning and environmental management.

-   Habitat modeling, species distribution, climate change vulnerability analysis.

-   Agronomy, agricultural planning, and conservation.

**How does it work?**

-   The data comes from networks of meteorological stations and satellites.

-   They are processed to create interpolated and raster maps at various resolutions.

-   They are distributed free of charge to researchers and international organizations, generally in raster formats.

R has specialized libraries such as raster, terra, & dismo to work with WorldClim climate data in raster format. Those variable could be raster files of climate variables (such as temperature, precipitation, humidity, etc.) in different resolutions and periods.

For my example I decided to change the country for mine.

## Cropping, masking, and aggregating raster data

```{r}
#| echo: true
#| message: true
#| warning: true

# Cropping
sextent <- terra::ext(delitos_data)
d <- terra::crop(d, sextent)
plot(d)
```

This topic permits us to crop a raster image covering only certain part regarding specific geographic stablished limits.

These limits in this case are given regarding crimes found in delitos_data for the area or areas of interest.

```{r}
#| echo: true
#| message: true
#| warning: true

# Masking
d <- terra::mask(d, vect(delitos_data))
plot(d)
```

Here we can see raster areas inside of interest geometries.

Applying masks we can view and work only with the data relevant to a certain area, cutting out the rest.

```{r}
#| echo: true
#| message: true
#| warning: true

# Aggregating
d <- terra::aggregate(d, fact = 20, fun = "mean", na.rm = TRUE)
plot(d)
```

This terra method groups or "resamples" a raster object (d) into larger blocks, combining several small cells into one, using a summarization operation.

It allows you to view the new grouped raster, showing the average values ​​at the lower resolution, making it easier to view patterns at a larger scale.

## Extracting raster values at points

```{r}
#| echo: true
#| message: true
#| warning: true

points <- st_coordinates(delitos_data)

# Convert SpatRaster to data frame for ggplot2
raster_df <- as.data.frame(d, xy = TRUE)
colnames(raster_df)[3] <- "tmin"  # Rename the temperature column

# Get the centroid coordinates
points_df <- st_coordinates(st_geometry(delitos_data))
points_df <- as.data.frame(points_df)
colnames(points_df) <- c("x", "y")

# Create the plots with ggplot2
ggplot() +
  geom_raster(data = raster_df, aes(x = x, y = y, fill = tmin)) +
  scale_fill_viridis_c(name = "Min Temp (C)") +
  geom_sf(data = delitos_data, color = "black", fill = "transparent") + # Plot the polygon borders
 geom_point(data = points_df, aes(x = x, y = y), color = "red", size = 0.01, shape = 1, alpha = 0.5) + # Plot the centroids as transparent circles
  labs(title = "Aggregated Minimum Temperature with Polygons and Centroids") +
  theme_minimal()
```

**Prompt:**

What is the purpose of extracting raster values ​​into points?

With extracting raster values we can visualize spatiotemporal data by combining raster and vector (point and polygon) information, with an emphasis on extracting and representing raster values ​​at specific points.

Display the raster variable (minimum temperature) on the same map as a heat map, superimposed on the polygon boundaries, and also mark the points or centroids of the areas of interest, allowing to identify how values ​​vary in different locations and how they relate to polygonal divisions and points of interest.

References: <https://mgimond.github.io/Spatial/spatial-autocorrelation.html>

::: {style="text-align: center; color: red;"}
THE END
:::
